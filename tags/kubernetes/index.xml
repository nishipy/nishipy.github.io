<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>kubernetes on nishipy notes</title><link>https://nishipy.github.io/tags/kubernetes/</link><description>Recent content in kubernetes on nishipy notes</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 28 Jan 2023 13:50:47 +0900</lastBuildDate><atom:link href="https://nishipy.github.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>Understanding PLEG with source code (1/2)</title><link>https://nishipy.github.io/p/understanding-pleg-with-source-code-1/2/</link><pubDate>Sat, 28 Jan 2023 13:50:47 +0900</pubDate><guid>https://nishipy.github.io/p/understanding-pleg-with-source-code-1/2/</guid><description>&lt;img src="https://nishipy.github.io/p/understanding-pleg-with-source-code-1/2/pleg.png" alt="Featured image of post Understanding PLEG with source code (1/2)" />&lt;p>Note: This article is a translation of &lt;a class="link" href="https://nishipy.com/archives/1958" target="_blank" rel="noopener"
>「kubeletのPLEGをソースコード読んで理解したかった」&lt;/a>.
&lt;a class="link" href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/" target="_blank" rel="noopener"
>Kubelet&lt;/a> has a lot of components, and one of the most important ones is PLEG, which stands for &amp;ldquo;Pod Lifecycle Event Generator&amp;rdquo;. In this article, we&amp;rsquo;ll look into the implementation to understand how PLEG works.&lt;/p>
&lt;h2 id="notready-nodes-due-to-pleg-is-not-healthy">NotReady nodes due to &amp;ldquo;PLEG is not healthy&amp;rdquo;&lt;/h2>
&lt;p>In Kubernetes, there are various causes for nodes to become NotReady state. &lt;a class="link" href="https://access.redhat.com/solutions/3258011" target="_blank" rel="noopener"
>For example&lt;/a>, a node may become NotReady with output like &lt;code>PLEG is not healthy: pleg was last seen active 3m20s ago; threshold is 3m0s. PLEG(←?!) was last seen active 3m20s ago; threshold is 3m0s It seems that the last time PLEG(←?!) was active was 3m20s ago; threshold is 3m0s &lt;/code>, so it seems that it was judged as abnormal because it exceeded the threshold of 3 minutes. I don&amp;rsquo;t know what it&amp;rsquo;s talking about, what the heck is PLEG&amp;hellip;&lt;/p>
&lt;h2 id="pleg">PLEG&lt;/h2>
&lt;p>PLEG is part of kubelet and stands for Pod Lifecycle Event Generator. An overview was found in &lt;a class="link" href="https://kubernetes.io/ja/docs/concepts/" target="_blank" rel="noopener"
>Concepts in Kubernetes Japanese documentation&lt;/a>. Below is its translation with DeepL.&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;gt; Once the desired state is set, the *Kubernetes control plane* using the Pod Lifecycle Event Generator (PLEG) functions to match the current state of the cluster to the desired state. To do so, Kubernetes automatically performs various tasks (e.g., starting or restarting containers, scaling the number of replicas of a particular application, etc.).
&lt;/code>&lt;/pre>&lt;p>Okay, that sounds very important. There is an embedded link to PLEG&amp;rsquo;s Design Proposal in this document, but it seems to be out of date and I cannot access it. Apparently, the old Design Proposal has been moved to &lt;a class="link" href="https://github.com/kubernetes/design-proposals-archive" target="_blank" rel="noopener"
>kubernetes/design-proposals-archive&lt;/a>. We can find the PLEG &lt;a class="link" href="https://github.com/kubernetes/design-proposals-archive/blob/main/node/pod-lifecycle-event-generator.md" target="_blank" rel="noopener"
>here&lt;/a>.&lt;/p>
&lt;p>It seems to be responsible for talking to Container Runtime and the main loop of kubelet, as shown below.&lt;/p>
&lt;blockquote>
&lt;p>&lt;img src="https://nishipy.github.io/p/understanding-pleg-with-source-code-1/2/pleg.png"
width="1206"
height="886"
srcset="https://nishipy.github.io/p/understanding-pleg-with-source-code-1/2/pleg_hu198fe2aa39d96d5ee57c047fcc9cf0ca_184542_480x0_resize_box_3.png 480w, https://nishipy.github.io/p/understanding-pleg-with-source-code-1/2/pleg_hu198fe2aa39d96d5ee57c047fcc9cf0ca_184542_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="136"
data-flex-basis="326px"
>
cited from: &lt;a class="link" href="https://github.com/kubernetes/design-proposals-archive/blob/main/node/pod-lifecycle-event-generator.md#overview" target="_blank" rel="noopener"
>https://github.com/kubernetes/design-proposals-archive/blob/main/node/pod-lifecycle-event-generator.md#overview&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h3 id="relisting-in-pleg">Relisting in PLEG&lt;/h3>
&lt;p>According to the chapter &lt;a class="link" href="https://github.com/kubernetes/design-proposals-archive/blob/main/node/pod-lifecycle-event-generator.md#detect-changes-in-container-states-via-relisting" target="_blank" rel="noopener"
>Detect Changes in Container States Via Relisting&lt;/a>, it is designed to detect container state changes by a process called &amp;ldquo;relisting&amp;rdquo;.&lt;/p>
&lt;ul>
&lt;li>PLEG relist all containers periodically to detect container state changes&lt;/li>
&lt;li>It helps prevent all Pod Workers from polling the container runtime in parallel&lt;/li>
&lt;li>Therefore only Pod Workers that need Sync will be launched, which is even more efficient&lt;/li>
&lt;/ul>
&lt;h2 id="pod-workers">Pod Workers&lt;/h2>
&lt;p>FYI, &lt;a class="link" href="https://github.com/kubernetes/kubernetes/blob/release-1.25/pkg/kubelet/pod_workers.go#L303-L378" target="_blank" rel="noopener"
>Pod Worker&lt;/a> is implemented here. It&amp;rsquo;s also a component of kubelet, and &lt;code>keeps track of operations on pods and ensures each pod is reconciled with the container runtime and other subsystems&lt;/code>.&lt;/p>
&lt;h2 id="runtime-pod-cache">Runtime Pod Cache&lt;/h2>
&lt;p>We also need to know the Runtime Pod Cache to look into PLEG.
The design proposal is &lt;a class="link" href="https://github.com/kubernetes/design-proposals-archive/blob/main/node/runtime-pod-cache.md" target="_blank" rel="noopener"
>here&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>&lt;img src="https://nishipy.github.io/p/understanding-pleg-with-source-code-1/2/runtimepodcache.png"
width="1214"
height="942"
srcset="https://nishipy.github.io/p/understanding-pleg-with-source-code-1/2/runtimepodcache_hu640db9cf4b083a2ee3efc8fe86f47dbc_196885_480x0_resize_box_3.png 480w, https://nishipy.github.io/p/understanding-pleg-with-source-code-1/2/runtimepodcache_hu640db9cf4b083a2ee3efc8fe86f47dbc_196885_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="128"
data-flex-basis="309px"
>
cited from: &lt;a class="link" href="https://github.com/kubernetes/design-proposals-archive/blob/main/node/runtime-pod-cache.md#runtime-pod-cache" target="_blank" rel="noopener"
>https://github.com/kubernetes/design-proposals-archive/blob/main/node/runtime-pod-cache.md#runtime-pod-cache&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>The diagram is almost the same as the one we saw for PLEG, but a box named &amp;ldquo;pod cache&amp;rdquo; is added between PLEG and Pod Workers.&lt;/p>
&lt;p>The Runtime Pod Cache is an in-memory cache that stores the state of all pods and is used to synchronize pods; it is managed by PLEG and acts as a Single Source of Truth (SSOT) for the internal pod status, so that kubelets do not need to directly query the container runtime directly.&lt;/p>
&lt;p>PLEG is responsible for updating the Pod Cache entries, keeping the cache up-to-date at all times. The design seems to be to process in the following order, generating and sending the corresponding Pod Lifecycle Event only when there is a change in the Pod.&lt;/p>
&lt;ol>
&lt;li>Detect change of container state&lt;/li>
&lt;li>Inspect the pod for details&lt;/li>
&lt;li>Update the pod cache with the new PodStatus&lt;/li>
&lt;/ol>
&lt;h2 id="more-information-and-source-code">More information and source code&lt;/h2>
&lt;p>Now that we have understood a little about PLEG, let&amp;rsquo;s look into it in more detail.&lt;/p>
&lt;p>First of all, the following article will help a lot to understand it with many diagrams and snippets of source codes. Please note that the information in it is as of Kubernetes 1.14.&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://developers.redhat.com/blog/2019/11/13/pod-lifecycle-event-generator-understanding-the-pleg-is-not-healthy-issue-in-kubernetes#" target="_blank" rel="noopener"
>Pod Lifecycle Event Generator: Understanding the &amp;ldquo;PLEG is not healthy&amp;rdquo; issue in Kubernetes | Red Hat Developer&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="in-the-next-article">In the next article&amp;hellip;&lt;/h2>
&lt;p>It might be sufficient to read the above for understanding, but the version of Kubernetes is a little old.
Therefore, I&amp;rsquo;ll read the source code of Kubernetes 1.25 in the next.&lt;/p></description></item><item><title>Resource allocation to Kind nodes</title><link>https://nishipy.github.io/p/resource-allocation-to-kind-nodes/</link><pubDate>Sat, 21 Jan 2023 10:51:42 +0900</pubDate><guid>https://nishipy.github.io/p/resource-allocation-to-kind-nodes/</guid><description>&lt;img src="https://nishipy.github.io/p/resource-allocation-to-kind-nodes/logo.png" alt="Featured image of post Resource allocation to Kind nodes " />&lt;p>Note: This article is a translation of &lt;a class="link" href="https://nishipy.com/archives/1793" target="_blank" rel="noopener"
>「kindでNodeに割り当てるリソースを定義する」&lt;/a>. I posted the original on Dec 18 2020, so some information might be old.&lt;/p>
&lt;p>There are some famous tools to create local Kubernetes clusters. &lt;a class="link" href="https://github.com/kubernetes-sigs/kind" target="_blank" rel="noopener"
>Kind&lt;/a> is one of them. In this post, I introduce the way to allocate compute resources(memory and/or CPU) to nodes of &lt;a class="link" href="https://github.com/kubernetes-sigs/kind" target="_blank" rel="noopener"
>Kind&lt;/a>.&lt;/p>
&lt;h2 id="adjust-resources-for-docker-desktop">Adjust resources for Docker Desktop&lt;/h2>
&lt;p>When using Kind, each node of a Kubernetes cluster is built as a Docker container. Therefore, first, configure the resources available to Docker in the Docker Desktop settings. Set them to about 4 CPU cores and 8GB of memory, for instance.&lt;/p>
&lt;p>&lt;img src="https://nishipy.github.io/p/resource-allocation-to-kind-nodes/docker-desktop-resources.png"
width="1556"
height="968"
srcset="https://nishipy.github.io/p/resource-allocation-to-kind-nodes/docker-desktop-resources_hu13aed214bcfc6718d8b2b4bfea2628f3_98503_480x0_resize_box_3.png 480w, https://nishipy.github.io/p/resource-allocation-to-kind-nodes/docker-desktop-resources_hu13aed214bcfc6718d8b2b4bfea2628f3_98503_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="160"
data-flex-basis="385px"
>&lt;/p>
&lt;h2 id="create-a-kind-configuration-file">Create a Kind configuration file&lt;/h2>
&lt;p>Next, we create a configuration file for Kind. You can get a general idea by looking at &lt;a class="link" href="https://kind.sigs.k8s.io/docs/user/quick-start/#configuring-your-kind-cluster" target="_blank" rel="noopener"
>Configuring Your kind Cluster&lt;/a> in the documentation. It is a brief description of the cluster.&lt;/p>
&lt;h3 id="define-multiple-nodes-in-a-cluster">Define multiple nodes in a cluster&lt;/h3>
&lt;p>For now, we will try to configure one Master and one Worker. All we need to do is to define the list under the &lt;code>nodes&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">kind.x-k8s.io/v1alpha4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">nodes&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">role&lt;/span>: &lt;span style="color:#ae81ff">control-plane&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">role&lt;/span>: &lt;span style="color:#ae81ff">worker&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="define-resources-for-nodes">Define resources for nodes&lt;/h3>
&lt;p>According to &lt;a class="link" href="https://github.com/kubernetes-sigs/kind/issues/1524" target="_blank" rel="noopener"
>this issue&lt;/a>, you can define the resource allocation since the Kind cluster is created with &lt;a class="link" href="https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/" target="_blank" rel="noopener"
>kubeadm&lt;/a>. However, it does not seem to be a formal method, as you can find a comment like &amp;ldquo;kubelet configuration object is not respected per node in kubeadm currently, only from init&amp;rdquo;.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">kind.x-k8s.io/v1alpha4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">nodes&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">role&lt;/span>: &lt;span style="color:#ae81ff">control-plane&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">kubeadmConfigPatches&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - |&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> kind: InitConfiguration
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> nodeRegistration:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> kubeletExtraArgs:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> system-reserved: memory=4Gi&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">role&lt;/span>: &lt;span style="color:#ae81ff">worker&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">kubeadmConfigPatches&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - |&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> kind: JoinConfiguration
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> nodeRegistration:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> kubeletExtraArgs:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> system-reserved: memory=4Gi&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!-- raw HTML omitted -->
&lt;p>&lt;!-- raw HTML omitted -->&lt;!-- raw HTML omitted -->&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;h3 id="create-a-cluster-with-the-configuration-file">Create a cluster with the configuration file&lt;/h3>
&lt;p>Use &lt;code>--config&lt;/code> option to specify the configuration file.&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kind create cluster --name=demo --config=$HOME/kind-config.yaml
Creating cluster &amp;#34;demo&amp;#34; ...
✓ Ensuring node image (kindest/node:v1.19.1)
✓ Preparing nodes
✓ Writing configuration
✓ Starting control-plane &amp;amp;#xfe0f;
✓ Installing CNI
✓ Installing StorageClass
✓ Joining worker nodes
Set kubectl context to &amp;#34;kind-demo&amp;#34;
You can now use your cluster with:
kubectl cluster-info --context kind-demo
Not sure what to do next? Check out &amp;lt;https://kind.sigs.k8s.io/docs/user/quick-start/&amp;gt;
&lt;/code>&lt;/pre>&lt;h3 id="confirm-the-resource-allocation">Confirm the resource allocation&lt;/h3>
&lt;p>You will see that it is properly allocated with &lt;code>kubectl describe node&lt;/code>.&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl get node
NAME STATUS ROLES AGE VERSION
demo-control-plane Ready master 76s v1.19.1
demo-worker Ready &amp;lt;none&amp;gt; 41s v1.19.1
$ kubectl describe node demo-control-plane
Name: demo-control-plane
[...]
Allocatable:
cpu: 4
ephemeral-storage: 61255492Ki
hugepages-1Gi: 0
hugepages-2Mi: 0
memory: 3958900Ki
pods: 110
[...]
$ kubectl describe node demo-worker
Name: demo-worker
[...]
Allocatable:
cpu: 4
ephemeral-storage: 61255492Ki
hugepages-1Gi: 0
hugepages-2Mi: 0
memory: 3958900Ki
pods: 110
&lt;/code>&lt;/pre></description></item></channel></rss>