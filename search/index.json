[{"content":"For the goal of this series or abstract of the PLEG and so on, please refer to the previous post Understanding PLEG with source code - Part 1. Now, let\u0026rsquo;s read the source code of PLEG! Note that the source code in the following part comes from Kubernetes 1.25.\npkg/kubelet/pleg/pleg.go The types of PodLifeCycleEvents in PLEG and their associated structures and interfaces are defined here. We have some events like ContainerStarted, ContainerDied, ContainerRemoved, PodSync and ContainerChanged.\n// PodLifeCycleEventType define the event type of pod life cycle events. type PodLifeCycleEventType string const ( // ContainerStarted - event type when the new state of container is running. ContainerStarted PodLifeCycleEventType = \u0026#34;ContainerStarted\u0026#34; // ContainerDied - event type when the new state of container is exited. ContainerDied PodLifeCycleEventType = \u0026#34;ContainerDied\u0026#34; // ContainerRemoved - event type when the old state of container is exited. ContainerRemoved PodLifeCycleEventType = \u0026#34;ContainerRemoved\u0026#34; // PodSync is used to trigger syncing of a pod when the observed change of // the state of the pod cannot be captured by any single event above. PodSync PodLifeCycleEventType = \u0026#34;PodSync\u0026#34; // ContainerChanged - event type when the new state of container is unknown. ContainerChanged PodLifeCycleEventType = \u0026#34;ContainerChanged\u0026#34; ) // PodLifecycleEvent is an event that reflects the change of the pod state. type PodLifecycleEvent struct { // The pod ID. ID types.UID // The type of the event. Type PodLifeCycleEventType // The accompanied data which varies based on the event type. // - ContainerStarted/ContainerStopped: the container name (string). // - All other event types: unused. Data interface{} } // PodLifecycleEventGenerator contains functions for generating pod life cycle events. type PodLifecycleEventGenerator interface { Start() Watch() chan *PodLifecycleEvent Healthy() (bool, error) } pkg/kubelet/pleg/generic.go The PLEG-related source code is mainly written in this file. The GenericPLEG structure looks as follows. As for the Cache, please refer to the Runtime Pod Cache described in the previous article. Looking at the comments in kubecontainer.Cache interface, Cache seems to store the PodStatus of all containers/pods visible to the container runtime.\ntype GenericPLEG struct { // The period for relisting. relistPeriod time.Duration // The container runtime. runtime kubecontainer.Runtime // The channel from which the subscriber listens events. eventChannel chan *PodLifecycleEvent // The internal cache for pod/container information. podRecords podRecords // Time of the last relisting. relistTime atomic.Value // Cache for storing the runtime states required for syncing pods. cache kubecontainer.Cache // For testability. clock clock.Clock // Pods that failed to have their status retrieved during a relist. These pods will be // retried during the next relisting. podsToReinspect map[types.UID]*kubecontainer.Pod } // Cache stores the PodStatus for the pods. It represents *all* the visible // pods/containers in the container runtime. All cache entries are at least as // new or newer than the global timestamp (set by UpdateTime()), while // individual entries may be slightly newer than the global timestamp. If a pod // has no states known by the runtime, Cache returns an empty PodStatus object // with ID populated. // // Cache provides two methods to retrieve the PodStatus: the non-blocking Get() // and the blocking GetNewerThan() method. The component responsible for // populating the cache is expected to call Delete() to explicitly free the // cache entries. type Cache interface { Get(types.UID) (*PodStatus, error) Set(types.UID, *PodStatus, error, time.Time) // GetNewerThan is a blocking call that only returns the status // when it is newer than the given time. GetNewerThan(types.UID, time.Time) (*PodStatus, error) Delete(types.UID) UpdateTime(time.Time) } The constant definition looks like this: relistThreshold is 3 minutes. When the Kubernetes/OpenShift nodes get NotReady, I saw this threshold in the error message PLEG is not healthy: pleg was last seen active XXmYYs ago; threshold is 3m0s.\n// plegContainerState has a one-to-one mapping to the // kubecontainer.State except for the non-existent state. This state // is introduced here to complete the state transition scenarios. type plegContainerState string const ( plegContainerRunning plegContainerState = \u0026#34;running\u0026#34; plegContainerExited plegContainerState = \u0026#34;exited\u0026#34; plegContainerUnknown plegContainerState = \u0026#34;unknown\u0026#34; plegContainerNonExistent plegContainerState = \u0026#34;non-existent\u0026#34; // The threshold needs to be greater than the relisting period + the // relisting time, which can vary significantly. Set a conservative // threshold to avoid flipping between healthy and unhealthy. relistThreshold = 3 * time.Minute ) *GenericPLEG.Healthy() The Healthy() method is a pointer method of the GenericPLEG structure. It checks if the PLEG itself is working properly. More specifically, if the interval between two relists exceeds 3 minutes (if elapsed \u0026gt; relistThreshold), the PLEG get failed. Then it reports the errors like PLEG is not healthy: pleg was last seen active XXmYYs ago; threshold is 3m0s.\nIt seems that elapsed will measure how much time has elapsed since the relistTime, which is a time of type time.Time and shows the time when the last relist was started).\n// Healthy check if PLEG work properly. // relistThreshold is the maximum interval between two relist. func (g *GenericPLEG) Healthy() (bool, error) { relistTime := g.getRelistTime() if relistTime.IsZero() { return false, fmt.Errorf(\u0026#34;pleg has yet to be successful\u0026#34;) } // Expose as metric so you can alert on `time()-pleg_last_seen_seconds \u0026gt; nn` metrics.PLEGLastSeen.Set(float64(relistTime.Unix())) elapsed := g.clock.Since(relistTime) if elapsed \u0026gt; relistThreshold { return false, fmt.Errorf(\u0026#34;pleg was last seen active %v ago; threshold is %v\u0026#34;, elapsed, relistThreshold) } return true, nil } // ~~ func (g *GenericPLEG) getRelistTime() time.Time { val := g.relistTime.Load() if val == nil { return time.Time{} } return val.(time.Time) } *GenericPLEG.relist() PLEG\u0026rsquo;s relisting is implemented in the relist() method. relist() queries the container runtime to obtain a list of pods/containers. It then compares it to the internal pods/containers and generates events accordingly. relist() performs the following steps:\nFirst, the current time is retrieved. After retrieving all pod information from the container runtime, relistTime is updated as the timestamp of the most recent start of relist(). g.runtime.GetPods(true) is passed with an argument true to retrieve the list of containers including exited and dead ones. The retrieved list is used to update the data in podRecords (internal cache of pod/container information).\n// relist queries the container runtime for list of pods/containers, compare // with the internal pods/containers, and generates events accordingly. func (g *GenericPLEG) relist() { klog.V(5).InfoS(\u0026#34;GenericPLEG: Relisting\u0026#34;) if lastRelistTime := g.getRelistTime(); !lastRelistTime.IsZero() { metrics.PLEGRelistInterval.Observe(metrics.SinceInSeconds(lastRelistTime)) } timestamp := g.clock.Now() defer func() { metrics.PLEGRelistDuration.Observe(metrics.SinceInSeconds(timestamp)) }() // Get all the pods. podList, err := g.runtime.GetPods(true) if err != nil { klog.ErrorS(err, \u0026#34;GenericPLEG: Unable to retrieve pods\u0026#34;) return } g.updateRelistTime(timestamp) pods := kubecontainer.Pods(podList) // update running pod and container count updateRunningPodAndContainerMetrics(pods) g.podRecords.setCurrent(pods) // ~~ PodRecord has the old and current fields, and the type of both is type kubecontainer.Pod. kubecontainer.Pod structure looks like as follows and contains information such as Pod name, Namespace, and list of containers.\ntype podRecord struct { old *kubecontainer.Pod current *kubecontainer.Pod } type podRecords map[types.UID]*podRecord // Pod is a group of containers. type Pod struct { // The ID of the pod, which can be used to retrieve a particular pod // from the pod list returned by GetPods(). ID types.UID // The name and namespace of the pod, which is readable by human. Name string Namespace string // List of containers that belongs to this pod. It may contain only // running containers, or mixed with dead ones (when GetPods(true)). Containers []*Container // List of sandboxes associated with this pod. The sandboxes are converted // to Container temporarily to avoid substantial changes to other // components. This is only populated by kuberuntime. // TODO: use the runtimeApi.PodSandbox type directly. Sandboxes []*Container } Now, let\u0026rsquo;s go back to the relist. For each pod, the current (current) and past (old) status of podRecords will be compared and events will be generated accordingly. The generated events are added to Map eventsByPodID. The pods are walked through one by one from this Map eventsByPodID, and if there is an event, then the PodCache will be updated.\nFrom case g.eventChannel \u0026lt;- events[i], we can see that the event will be sent if the PodLyifecycleEvent channel has room.\nfunc (g *GenericPLEG) relist() { // ~~ // Compare the old and the current pods, and generate events. eventsByPodID := map[types.UID][]*PodLifecycleEvent{} for pid := range g.podRecords { oldPod := g.podRecords.getOld(pid) pod := g.podRecords.getCurrent(pid) // Get all containers in the old and the new pod. allContainers := getContainersFromPods(oldPod, pod) for _, container := range allContainers { events := computeEvents(oldPod, pod, \u0026amp;container.ID) for _, e := range events { updateEvents(eventsByPodID, e) } } } var needsReinspection map[types.UID]*kubecontainer.Pod if g.cacheEnabled() { needsReinspection = make(map[types.UID]*kubecontainer.Pod) } // If there are events associated with a pod, we should update the // podCache. for pid, events := range eventsByPodID { pod := g.podRecords.getCurrent(pid) if g.cacheEnabled() { // updateCache() will inspect the pod and update the cache. If an // error occurs during the inspection, we want PLEG to retry again // in the next relist. To achieve this, we do not update the // associated podRecord of the pod, so that the change will be // detect again in the next relist. // TODO: If many pods changed during the same relist period, // inspecting the pod and getting the PodStatus to update the cache // serially may take a while. We should be aware of this and // parallelize if needed. if err := g.updateCache(pod, pid); err != nil { // Rely on updateCache calling GetPodStatus to log the actual error. klog.V(4).ErrorS(err, \u0026#34;PLEG: Ignoring events for pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KRef(pod.Namespace, pod.Name)) // make sure we try to reinspect the pod during the next relisting needsReinspection[pid] = pod continue } else { // this pod was in the list to reinspect and we did so because it had events, so remove it // from the list (we don\u0026#39;t want the reinspection code below to inspect it a second time in // this relist execution) delete(g.podsToReinspect, pid) } } // Update the internal storage and send out the events. g.podRecords.update(pid) // Map from containerId to exit code; used as a temporary cache for lookup containerExitCode := make(map[string]int) func (g *GenericPLEG) relist() { // ~~ // Compare the old and the current pods, and generate events. eventsByPodID := map[types.UID][]*PodLifecycleEvent{} for pid := range g.podRecords { oldPod := g.podRecords.getOld(pid) pod := g.podRecords.getCurrent(pid) // Get all containers in the old and the new pod. allContainers := getContainersFromPods(oldPod, pod) for _, container := range allContainers { events := computeEvents(oldPod, pod, \u0026amp;container.ID) for _, e := range events { updateEvents(eventsByPodID, e) } } } var needsReinspection map[types.UID]*kubecontainer.Pod if g.cacheEnabled() { needsReinspection = make(map[types.UID]*kubecontainer.Pod) } // If there are events associated with a pod, we should update the // podCache. for pid, events := range eventsByPodID { pod := g.podRecords.getCurrent(pid) if g.cacheEnabled() { // updateCache() will inspect the pod and update the cache. If an // error occurs during the inspection, we want PLEG to retry again // in the next relist. To achieve this, we do not update the // associated podRecord of the pod, so that the change will be // detect again in the next relist. // TODO: If many pods changed during the same relist period, // inspecting the pod and getting the PodStatus to update the cache // serially may take a while. We should be aware of this and // parallelize if needed. if err := g.updateCache(pod, pid); err != nil { // Rely on updateCache calling GetPodStatus to log the actual error. klog.V(4).ErrorS(err, \u0026#34;PLEG: Ignoring events for pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KRef(pod.Namespace, pod.Name)) // make sure we try to reinspect the pod during the next relisting needsReinspection[pid] = pod continue } else { // this pod was in the list to reinspect and we did so because it had events, so remove it // from the list (we don\u0026#39;t want the reinspection code below to inspect it a second time in // this relist execution) delete(g.podsToReinspect, pid) } } // Update the internal storage and send out the events. g.podRecords.update(pid) // Map from containerId to exit code; used as a temporary cache for lookup containerExitCode := make(map[string]int) for i := range events { // Filter out events that are not reliable and no other components use yet. if events[i].Type == ContainerChanged { continue } select { case g.eventChannel \u0026lt;- events[i]: default: metrics.PLEGDiscardEvents.Inc() klog.ErrorS(nil, \u0026#34;Event channel is full, discard this relist() cycle event\u0026#34;) } // Log exit code of containers when they finished in a particular event if events[i].Type == ContainerDied { // Fill up containerExitCode map for ContainerDied event when first time appeared if len(containerExitCode) == 0 \u0026amp;\u0026amp; pod != nil \u0026amp;\u0026amp; g.cache != nil { // Get updated podStatus status, err := g.cache.Get(pod.ID) if err == nil { for _, containerStatus := range status.ContainerStatuses { containerExitCode[containerStatus.ID.ID] = containerStatus.ExitCode } } } if containerID, ok := events[i].Data.(string); ok { if exitCode, ok := containerExitCode[containerID]; ok \u0026amp;\u0026amp; pod != nil { klog.V(2).InfoS(\u0026#34;Generic (PLEG): container finished\u0026#34;, \u0026#34;podID\u0026#34;, pod.ID, \u0026#34;containerID\u0026#34;, containerID, \u0026#34;exitCode\u0026#34;, exitCode) } } } } } if g.cacheEnabled() { // reinspect any pods that failed inspection during the previous relist if len(g.podsToReinspect) \u0026gt; 0 { klog.V(5).InfoS(\u0026#34;GenericPLEG: Reinspecting pods that previously failed inspection\u0026#34;) for pid, pod := range g.podsToReinspect { if err := g.updateCache(pod, pid); err != nil { // Rely on updateCache calling GetPodStatus to log the actual error. klog.V(5).ErrorS(err, \u0026#34;PLEG: pod failed reinspection\u0026#34;, \u0026#34;pod\u0026#34;, klog.KRef(pod.Namespace, pod.Name)) needsReinspection[pid] = pod } } } // Update the cache timestamp. This needs to happen *after* // all pods have been properly updated in the cache. g.cache.UpdateTime(timestamp) } // make sure we retain the list of pods that need reinspecting the next time relist is called g.podsToReinspect = needsReinspection } Pod Lifecycle Event generation is performed by generateEvents() below. As you can see from the first if statement, if the state is the same as the previous state, no event will be generated.\nfunc generateEvents(podID types.UID, cid string, oldState, newState plegContainerState) []*PodLifecycleEvent { if newState == oldState { return nil } klog.V(4).InfoS(\u0026#34;GenericPLEG\u0026#34;, \u0026#34;podUID\u0026#34;, podID, \u0026#34;containerID\u0026#34;, cid, \u0026#34;oldState\u0026#34;, oldState, \u0026#34;newState\u0026#34;, newState) switch newState { case plegContainerRunning: return []*PodLifecycleEvent{{ID: podID, Type: ContainerStarted, Data: cid}} case plegContainerExited: return []*PodLifecycleEvent{{ID: podID, Type: ContainerDied, Data: cid}} case plegContainerUnknown: return []*PodLifecycleEvent{{ID: podID, Type: ContainerChanged, Data: cid}} case plegContainerNonExistent: switch oldState { case plegContainerExited: // We already reported that the container died before. return []*PodLifecycleEvent{{ID: podID, Type: ContainerRemoved, Data: cid}} default: return []*PodLifecycleEvent{{ID: podID, Type: ContainerDied, Data: cid}, {ID: podID, Type: ContainerRemoved, Data: cid}} } default: panic(fmt.Sprintf(\u0026#34;unrecognized container state: %v\u0026#34;, newState)) } } *GenericPLEG.Start() In the Start() method, the relist() method will be executed in goroutine. The wait.Until() is used here, hence it loops at relistPeriod (= 1 second) intervals.\n// Start spawns a goroutine to relist periodically. func (g *GenericPLEG) Start() { go wait.Until(g.relist, g.relistPeriod, wait.NeverStop) } *GenericPLEG.Watch() Watch() is a method that returns a channel for PodLifecycleEvents. It returns the channel *PodLifecycleEvent, and the kubelet receives events from this channel and performs the Pod synchronizations as appropriate.\n// Watch returns a channel from which the subscriber can receive PodLifecycleEvent // events. // TODO: support multiple subscribers. func (g *GenericPLEG) Watch() chan *PodLifecycleEvent { return g.eventChannel } pkg/kubelet/kubelet.go Furthermore, let\u0026rsquo;s see how PLEG is called on the kubelet side. There are some PLEG-related constants: the relist interval is 1 second, and the capacity of the channel for PodLifecycleEvents looks 1000.\nconst ( //~~ // Capacity of the channel for receiving pod lifecycle events. This number // is a bit arbitrary and may be adjusted in the future. plegChannelCapacity = 1000 // Generic PLEG relies on relisting for discovering container events. // A longer period means that kubelet will take longer to detect container // changes and to update pod status. On the other hand, a shorter period // will cause more frequent relisting (e.g., container runtime operations), // leading to higher cpu usage. // Note that even though we set the period to 1s, the relisting itself can // take more than 1s to finish if the container runtime responds slowly // and/or when there are many container changes in one cycle. plegRelistPeriod = time.Second * 1 //~~ ) The NewMainKubelet() will instantiate a new Kubelet object along with all necessary internal modules. The PLEG-related processing is as follows. After instantiating the GenericPLEG object in NewGenericPLEG(), it will be added to the health check mechanism in the main loop of the kubelet. As a result, we sometimes see the error messages like the PLEG is not healthy: pleg was last seen active XXmYYs ago; threshold is 3m0s in the kubelet log.\n// NewMainKubelet instantiates a new Kubelet object along with all the required internal modules. // No initialization of Kubelet and its modules should happen here. func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, crOptions *config.ContainerRuntimeOptions, hostname string, hostnameOverridden bool, // ~~ seccompDefault bool, ) (*Kubelet, error) { // ~~ klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, klet.podCache, clock.RealClock{}) klet.runtimeState = newRuntimeState(maxWaitForContainerRuntime) klet.runtimeState.addHealthCheck(\u0026#34;PLEG\u0026#34;, klet.pleg.Healthy) // ~~ } You can see that PLEG is also started with kl.pleg.Start() in the Run() method on the kubelet side.\n// Run starts the kubelet reacting to config updates func (kl *Kubelet) Run(updates \u0026lt;-chan kubetypes.PodUpdate) { if kl.logServer == nil { kl.logServer = http.StripPrefix(\u0026#34;/logs/\u0026#34;, http.FileServer(http.Dir(\u0026#34;/var/log/\u0026#34;))) } if kl.kubeClient == nil { klog.InfoS(\u0026#34;No API server defined - no node status update will be sent\u0026#34;) } // Start the cloud provider sync manager if kl.cloudResourceSyncManager != nil { go kl.cloudResourceSyncManager.Run(wait.NeverStop) } if err := kl.initializeModules(); err != nil { kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error()) klog.ErrorS(err, \u0026#34;Failed to initialize internal modules\u0026#34;) os.Exit(1) } // Start volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) if kl.kubeClient != nil { // Introduce some small jittering to ensure that over time the requests won\u0026#39;t start // accumulating at approximately the same time from the set of nodes due to priority and // fairness effect. go wait.JitterUntil(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, 0.04, true, wait.NeverStop) go kl.fastStatusUpdateOnce() // start syncing lease go kl.nodeLeaseController.Run(wait.NeverStop) } go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) // Set up iptables util rules if kl.makeIPTablesUtilChains { kl.initNetworkUtil() } // Start component sync loops. kl.statusManager.Start() // Start syncing RuntimeClasses if enabled. if kl.runtimeClassManager != nil { kl.runtimeClassManager.Start(wait.NeverStop) } // Start the pod lifecycle event generator. kl.pleg.Start() kl.syncLoop(updates, kl) } At the end of the Run() method, the syncLoop() method is also called. The plegCh := kl.pleg.Watch() part in syncLoop() obtains a channel to read PLEG updates and receives events generated by PLEG via the channel.\n// syncLoop is the main loop for processing changes. It watches for changes from // three channels (file, apiserver, and http) and creates a union of them. For // any new change seen, will run a sync against desired state and running state. If // no changes are seen to the configuration, will synchronize the last known desired // state every sync-frequency seconds. Never returns. func (kl *Kubelet) syncLoop(updates \u0026lt;-chan kubetypes.PodUpdate, handler SyncHandler) { klog.InfoS(\u0026#34;Starting kubelet main sync loop\u0026#34;) // The syncTicker wakes up kubelet to checks if there are any pod workers // that need to be sync\u0026#39;d. A one-second period is sufficient because the // sync interval is defaulted to 10s. syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) duration := base // Responsible for checking limits in resolv.conf // The limits do not have anything to do with individual pods // Since this is called in syncLoop, we don\u0026#39;t need to call it anywhere else if kl.dnsConfigurer != nil \u0026amp;\u0026amp; kl.dnsConfigurer.ResolverConfig != \u0026#34;\u0026#34; { kl.dnsConfigurer.CheckLimitsForResolvConf() } for { if err := kl.runtimeState.runtimeErrors(); err != nil { klog.ErrorS(err, \u0026#34;Skipping pod synchronization\u0026#34;) // exponential backoff time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue } // reset backoff if we have a success duration = base kl.syncLoopMonitor.Store(kl.clock.Now()) if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) { break } kl.syncLoopMonitor.Store(kl.clock.Now()) } } syncLoopIteration() is a method that reads various channels and dispatches them to a given Handler. The plegCh channel is described as being used for Runtime Cache updates and Pod synchronization. For example, if the kubelet receives the event PLEG\u0026rsquo;s ContainerDied (= the latest container state is Exited), the kubelet will delete the container instance in question in the pod via cleanUpContainersInPod().\n// syncLoopIteration reads from various channels and dispatches pods to the // given handler. // // Arguments: // 1. configCh: a channel to read config events from // 2. handler: the SyncHandler to dispatch pods to // 3. syncCh: a channel to read periodic sync events from // 4. housekeepingCh: a channel to read housekeeping events from // 5. plegCh: a channel to read PLEG updates from // // Events are also read from the kubelet liveness manager\u0026#39;s update channel. // // The workflow is to read from one of the channels, handle that event, and // update the timestamp in the sync loop monitor. // // Here is an appropriate place to note that despite the syntactical // similarity to the switch statement, the case statements in a select are // evaluated in a pseudorandom order if there are multiple channels ready to // read from when the select is evaluated. In other words, case statements // are evaluated in random order, and you can not assume that the case // statements evaluate in order if multiple channels have events. // // With that in mind, in truly no particular order, the different channels // are handled as follows: // // - configCh: dispatch the pods for the config change to the appropriate // handler callback for the event type // - plegCh: update the runtime cache; sync pod // - syncCh: sync all pods waiting for sync // - housekeepingCh: trigger cleanup of pods // - health manager: sync pods that have failed or in which one or more // containers have failed health checks func (kl *Kubelet) syncLoopIteration(configCh \u0026lt;-chan kubetypes.PodUpdate, handler SyncHandler, syncCh \u0026lt;-chan time.Time, housekeepingCh \u0026lt;-chan time.Time, plegCh \u0026lt;-chan *pleg.PodLifecycleEvent) bool { // ~~ case e := \u0026lt;-plegCh: if e.Type == pleg.ContainerStarted { // record the most recent time we observed a container start for this pod. // this lets us selectively invalidate the runtimeCache when processing a delete for this pod // to make sure we don\u0026#39;t miss handling graceful termination for containers we reported as having started. kl.lastContainerStartedTime.Add(e.ID, time.Now()) } if isSyncPodWorthy(e) { // PLEG event for a pod; sync it. if pod, ok := kl.podManager.GetPodByUID(e.ID); ok { klog.V(2).InfoS(\u0026#34;SyncLoop (PLEG): event for pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;event\u0026#34;, e) handler.HandlePodSyncs([]*v1.Pod{pod}) } else { // If the pod no longer exists, ignore the event. klog.V(4).InfoS(\u0026#34;SyncLoop (PLEG): pod does not exist, ignore irrelevant event\u0026#34;, \u0026#34;event\u0026#34;, e) } } if e.Type == pleg.ContainerDied { if containerID, ok := e.Data.(string); ok { kl.cleanUpContainersInPod(e.ID, containerID) } } // ~~ pkg/kubelet/runtime.go Let\u0026rsquo;s also look at where the PLEG-related health check is added to the kubelet runtime health check with klet.runtimeState.The health check functions, which were added by addHealthCheck(), will be called with the for statement and evaluated. When the health check fails, the error message will be shown with a format like fmt.Errorf(\u0026quot;%s is not healthy: %v\u0026quot;, hc.name, err). Yeah, now we can see where PLEG is not healthy: pleg was last seen active XXmYYs ago; threshold is 3m comes from!\n// A health check function should be efficient and not rely on external // components (e.g., container runtime). type healthCheckFnType func() (bool, error) type healthCheck struct { name string fn healthCheckFnType } func (s *runtimeState) addHealthCheck(name string, f healthCheckFnType) { s.Lock() defer s.Unlock() s.healthChecks = append(s.healthChecks, \u0026amp;healthCheck{name: name, fn: f}) } func (s *runtimeState) runtimeErrors() error { s.RLock() defer s.RUnlock() errs := []error{} if s.lastBaseRuntimeSync.IsZero() { errs = append(errs, errors.New(\u0026#34;container runtime status check may not have completed yet\u0026#34;)) } else if !s.lastBaseRuntimeSync.Add(s.baseRuntimeSyncThreshold).After(time.Now()) { errs = append(errs, errors.New(\u0026#34;container runtime is down\u0026#34;)) } for _, hc := range s.healthChecks { if ok, err := hc.fn(); !ok { errs = append(errs, fmt.Errorf(\u0026#34;%s is not healthy: %v\u0026#34;, hc.name, err)) } } if s.runtimeError != nil { errs = append(errs, s.runtimeError) } return utilerrors.NewAggregate(errs) } What\u0026rsquo;s next in PLEG? I found the KEP Kubelet Evented PLEG for Better Performance. It seems still under development for now.\nIsuue: Kubelet Evented PLEG for Better Performance - GitHub References \u0026ldquo;PLEG is not healthy\u0026rdquo; errors on OpenShift nodes. - Red Hat Customer Portal コンセプト - Kubernetes Document Pod Lifecycle Event Generator: Understanding the “PLEG is not healthy” issue in Kubernetes - Red Hat Developer kubernetes/design-proposals-archive - GitHub kubernetes/pkg/kubelet/pleg at release-1.25 - GitHub Isuue: Kubelet Evented PLEG for Better Performance - GitHub ","date":"2023-03-24T09:32:47+09:00","image":"https://nishipy.github.io/p/understanding-pleg-with-source-code-part-2/pleg_hu198fe2aa39d96d5ee57c047fcc9cf0ca_184542_120x120_fill_box_smart1_3.png","permalink":"https://nishipy.github.io/p/understanding-pleg-with-source-code-part-2/","title":"Understanding PLEG with source code - Part 2"},{"content":"Note: This article is a translation of 「kubeletのPLEGをソースコード読んで理解したかった」. Kubelet has a lot of components, and one of the most important ones is PLEG, which stands for \u0026ldquo;Pod Lifecycle Event Generator\u0026rdquo;. In this article, we\u0026rsquo;ll look into the implementation to understand how PLEG works.\nNotReady nodes due to \u0026ldquo;PLEG is not healthy\u0026rdquo; In Kubernetes, there are various causes for nodes to become NotReady state. For example, a node may become NotReady with output like PLEG is not healthy: pleg was last seen active 3m20s ago; threshold is 3m0s. PLEG(←?!) was last seen active 3m20s ago; threshold is 3m0s It seems that the last time PLEG(←?!) was active was 3m20s ago; threshold is 3m0s , so it seems that it was judged as abnormal because it exceeded the threshold of 3 minutes. I don\u0026rsquo;t know what it\u0026rsquo;s talking about, what the heck is PLEG\u0026hellip;\nPLEG PLEG is part of kubelet and stands for Pod Lifecycle Event Generator. An overview was found in Concepts in Kubernetes Japanese documentation. Below is its translation with DeepL.\n\u0026gt; Once the desired state is set, the *Kubernetes control plane* using the Pod Lifecycle Event Generator (PLEG) functions to match the current state of the cluster to the desired state. To do so, Kubernetes automatically performs various tasks (e.g., starting or restarting containers, scaling the number of replicas of a particular application, etc.). Okay, that sounds very important. There is an embedded link to PLEG\u0026rsquo;s Design Proposal in this document, but it seems to be out of date and I cannot access it. Apparently, the old Design Proposal has been moved to kubernetes/design-proposals-archive. We can find the PLEG here.\nIt seems to be responsible for talking to Container Runtime and the main loop of kubelet, as shown below.\ncited from: https://github.com/kubernetes/design-proposals-archive/blob/main/node/pod-lifecycle-event-generator.md#overview\n非推奨\nRelisting in PLEG According to the chapter Detect Changes in Container States Via Relisting, it is designed to detect container state changes by a process called \u0026ldquo;relisting\u0026rdquo;.\nPLEG relist all containers periodically to detect container state changes It helps prevent all Pod Workers from polling the container runtime in parallel Therefore only Pod Workers that need Sync will be launched, which is even more efficient Pod Workers FYI, Pod Worker is implemented here. It\u0026rsquo;s also a component of kubelet, and keeps track of operations on pods and ensures each pod is reconciled with the container runtime and other subsystems.\nRuntime Pod Cache We also need to know the Runtime Pod Cache to look into PLEG. The design proposal is here.\ncited from: https://github.com/kubernetes/design-proposals-archive/blob/main/node/runtime-pod-cache.md#runtime-pod-cache\nThe diagram is almost the same as the one we saw for PLEG, but a box named \u0026ldquo;pod cache\u0026rdquo; is added between PLEG and Pod Workers.\nThe Runtime Pod Cache is an in-memory cache that stores the state of all pods and is used to synchronize pods; it is managed by PLEG and acts as a Single Source of Truth (SSOT) for the internal pod status, so that kubelets do not need to directly query the container runtime directly.\nPLEG is responsible for updating the Pod Cache entries, keeping the cache up-to-date at all times. The design seems to be to process in the following order, generating and sending the corresponding Pod Lifecycle Event only when there is a change in the Pod.\nDetect change of container state Inspect the pod for details Update the pod cache with the new PodStatus More information and source code Now that we have understood a little about PLEG, let\u0026rsquo;s look into it in more detail.\nFirst of all, the following article will help a lot to understand it with many diagrams and snippets of source codes. Please note that the information in it is as of Kubernetes 1.14.\nPod Lifecycle Event Generator: Understanding the \u0026ldquo;PLEG is not healthy\u0026rdquo; issue in Kubernetes | Red Hat Developer In the next article\u0026hellip; It might be sufficient to read the above for understanding, but the version of Kubernetes is a little old. Therefore, I\u0026rsquo;ll read the source code of Kubernetes 1.25 in the next.\n","date":"2023-01-28T13:50:47+09:00","image":"https://nishipy.github.io/p/understanding-pleg-with-source-code-part-1/pleg_hu198fe2aa39d96d5ee57c047fcc9cf0ca_184542_120x120_fill_box_smart1_3.png","permalink":"https://nishipy.github.io/p/understanding-pleg-with-source-code-part-1/","title":"Understanding PLEG with source code - Part 1"},{"content":"Note: This article is a translation of 「kindでNodeに割り当てるリソースを定義する」. I posted the original on Dec 18 2020, so some information might be old.\nThere are some famous tools to create local Kubernetes clusters. Kind is one of them. In this post, I introduce the way to allocate compute resources(memory and/or CPU) to nodes of Kind.\nAdjust resources for Docker Desktop When using Kind, each node of a Kubernetes cluster is built as a Docker container. Therefore, first, configure the resources available to Docker in the Docker Desktop settings. Set them to about 4 CPU cores and 8GB of memory, for instance.\nCreate a Kind configuration file Next, we create a configuration file for Kind. You can get a general idea by looking at Configuring Your kind Cluster in the documentation. It is a brief description of the cluster.\nDefine multiple nodes in a cluster For now, we will try to configure one Master and one Worker. All we need to do is to define the list under the nodes.\nkind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane - role: worker Define resources for nodes According to this issue, you can define the resource allocation since the Kind cluster is created with kubeadm. However, it does not seem to be a formal method, as you can find a comment like \u0026ldquo;kubelet configuration object is not respected per node in kubeadm currently, only from init\u0026rdquo;.\nkind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: system-reserved: memory=4Gi - role: worker kubeadmConfigPatches: - | kind: JoinConfiguration nodeRegistration: kubeletExtraArgs: system-reserved: memory=4Gi Create a cluster with the configuration file Use --config option to specify the configuration file.\n$ kind create cluster --name=demo --config=$HOME/kind-config.yaml Creating cluster \u0026#34;demo\u0026#34; ... ✓ Ensuring node image (kindest/node:v1.19.1) ✓ Preparing nodes ✓ Writing configuration ✓ Starting control-plane \u0026amp;#xfe0f; ✓ Installing CNI ✓ Installing StorageClass ✓ Joining worker nodes Set kubectl context to \u0026#34;kind-demo\u0026#34; You can now use your cluster with: kubectl cluster-info --context kind-demo Not sure what to do next? Check out \u0026lt;https://kind.sigs.k8s.io/docs/user/quick-start/\u0026gt; Confirm the resource allocation You will see that it is properly allocated with kubectl describe node.\n$ kubectl get node\tNAME STATUS ROLES AGE VERSION\tdemo-control-plane Ready master 76s v1.19.1\tdemo-worker Ready \u0026lt;none\u0026gt; 41s v1.19.1\t$ kubectl describe node demo-control-plane\tName: demo-control-plane\t[...]\tAllocatable:\tcpu: 4\tephemeral-storage: 61255492Ki\thugepages-1Gi: 0\thugepages-2Mi: 0\tmemory: 3958900Ki\tpods: 110\t[...]\t$ kubectl describe node demo-worker\tName: demo-worker\t[...]\tAllocatable:\tcpu: 4\tephemeral-storage: 61255492Ki\thugepages-1Gi: 0\thugepages-2Mi: 0\tmemory: 3958900Ki\tpods: 110\t","date":"2023-01-21T10:51:42+09:00","image":"https://nishipy.github.io/p/resource-allocation-to-kind-nodes/logo_hu3178206b939ba8d9865acabeaf6111f4_85649_120x120_fill_box_smart1_3.png","permalink":"https://nishipy.github.io/p/resource-allocation-to-kind-nodes/","title":"Resource allocation to Kind nodes "},{"content":"I used to write articles on the WordPress-based blog site, but in 2023 I am trying to migrate to Hugo.\nHugo has a variety of great themes, but I\u0026rsquo;ve decided to use the Hugo Theme Stack.\nWhat is Hugo Theme Stack Hugo Theme Stack is a Card-style Hugo theme designed for bloggers, and one of the best Hugo themes I\u0026rsquo;ve ever seen. It has nice designs, a dark/right theme, and so on.\nHowever, if I understand correctly, it doesn\u0026rsquo;t have a feature to add social media share buttons at the moment I wrote this article. Then, I tried to add them at the bottom of the articles.\nSocial Media Share Buttons for a Hugo Website Please find Social Media Share Buttons for a Hugo Website, which is a perfect guide on how to add social media share buttons for the Hugo themes.\nThe original code can be generated with Sharingbuttons.io. In addition, you have to replace the link and text with {{ .Title }} and {{ .Permalink }} if you want to share the page where you are. Here is my example for Twitter.\n\u0026lt;a class=\u0026#34;resp-sharing-button__link\u0026#34; href=\u0026#34;https://twitter.com/intent/tweet/?text={{ .Title }}\u0026amp;amp;url={{ .Permalink }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34; aria-label=\u0026#34;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small\u0026#34;\u0026gt;\u0026lt;div aria-hidden=\u0026#34;true\u0026#34; class=\u0026#34;resp-sharing-button__icon resp-sharing-button__icon--solid\u0026#34;\u0026gt; \u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5 0-4.55 2.04-4.55 4.54 0 .36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3 0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35 0 12.92-6.92 12.92-12.93 0-.2 0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; Add Social Media Share Buttons for Hugo Theme Stack Referencing the procedures in the previous part, now we can add the buttons for Hugo Theme Stack.\nassets/scss/custom.scss Update custom.scss as follows to support social media share buttons. You just cut and paste the code generated by Sharingbuttons.io.\n// Social media sharing buttons .resp-sharing-button__link, .resp-sharing-button__icon { display: inline-block } .resp-sharing-button__link { text-decoration: none; color: #fff; margin: 0.5em } .resp-sharing-button { border-radius: 5px; transition: 25ms ease-out; padding: 0.5em 0.75em; font-family: Helvetica Neue,Helvetica,Arial,sans-serif } .resp-sharing-button__icon svg { width: 1em; height: 1em; margin-right: 0.4em; vertical-align: top } .resp-sharing-button--small svg { margin: 0; vertical-align: middle } /* Non solid icons get a stroke */ .resp-sharing-button__icon { stroke: #fff; fill: none } /* Solid icons get a fill */ .resp-sharing-button__icon--solid, .resp-sharing-button__icon--solidcircle { fill: #fff; stroke: none } .resp-sharing-button--twitter { background-color: #55acee } .resp-sharing-button--twitter:hover { background-color: #2795e9 } .resp-sharing-button--pinterest { background-color: #bd081c } .resp-sharing-button--pinterest:hover { background-color: #8c0615 } .resp-sharing-button--facebook { background-color: #3b5998 } .resp-sharing-button--facebook:hover { background-color: #2d4373 } .resp-sharing-button--tumblr { background-color: #35465C } .resp-sharing-button--tumblr:hover { background-color: #222d3c } .resp-sharing-button--reddit { background-color: #5f99cf } .resp-sharing-button--reddit:hover { background-color: #3a80c1 } .resp-sharing-button--google { background-color: #dd4b39 } .resp-sharing-button--google:hover { background-color: #c23321 } .resp-sharing-button--linkedin { background-color: #0077b5 } .resp-sharing-button--linkedin:hover { background-color: #046293 } .resp-sharing-button--email { background-color: #777 } .resp-sharing-button--email:hover { background-color: #5e5e5e } .resp-sharing-button--xing { background-color: #1a7576 } .resp-sharing-button--xing:hover { background-color: #114c4c } .resp-sharing-button--whatsapp { background-color: #25D366 } .resp-sharing-button--whatsapp:hover { background-color: #1da851 } .resp-sharing-button--hackernews { background-color: #FF6600 } .resp-sharing-button--hackernews:hover, .resp-sharing-button--hackernews:focus { background-color: #FB6200 } .resp-sharing-button--vk { background-color: #507299 } .resp-sharing-button--vk:hover { background-color: #43648c } .resp-sharing-button--facebook { background-color: #3b5998; border-color: #3b5998; } .resp-sharing-button--facebook:hover, .resp-sharing-button--facebook:active { background-color: #2d4373; border-color: #2d4373; } .resp-sharing-button--twitter { background-color: #55acee; border-color: #55acee; } .resp-sharing-button--twitter:hover, .resp-sharing-button--twitter:active { background-color: #2795e9; border-color: #2795e9; } .resp-sharing-button--linkedin { background-color: #0077b5; border-color: #0077b5; } .resp-sharing-button--linkedin:hover, .resp-sharing-button--linkedin:active { background-color: #046293; border-color: #046293; } .resp-sharing-button--reddit { background-color: #5f99cf; border-color: #5f99cf; } .resp-sharing-button--reddit:hover, .resp-sharing-button--reddit:active { background-color: #3a80c1; border-color: #3a80c1; } assets/scss/partials/layout/article.scss Add the definition of .share-buttons section in .article-footer. Here, it is the same as .article-copyright and .article-lastmod.\n//~~ .article-lastmod, .share-buttons { a { color: var(--body-text-color); } layouts/partials/article/components/share-buttons.html Next, we add a new partial template for share buttons like this. The original one is available at Sharingbuttons.io again. As mentioned above, the link and text are replaced with {{ .Title }} and {{ .Permalink }}.\n\u0026lt;section class=\u0026#34;share-buttons\u0026#34;\u0026gt; \u0026lt;!-- Sharingbutton Twitter --\u0026gt; \u0026lt;a class=\u0026#34;resp-sharing-button__link\u0026#34; href=\u0026#34;https://twitter.com/intent/tweet/?text={{ .Title }}\u0026amp;amp;url={{ .Permalink }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34; aria-label=\u0026#34;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small\u0026#34;\u0026gt;\u0026lt;div aria-hidden=\u0026#34;true\u0026#34; class=\u0026#34;resp-sharing-button__icon resp-sharing-button__icon--solid\u0026#34;\u0026gt; \u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5 0-4.55 2.04-4.55 4.54 0 .36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3 0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35 0 12.92-6.92 12.92-12.93 0-.2 0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;!-- Sharingbutton Facebook --\u0026gt; \u0026lt;a class=\u0026#34;resp-sharing-button__link\u0026#34; href=\u0026#34;https://facebook.com/sharer/sharer.php?u={{ .Permalink }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34; aria-label=\u0026#34;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;resp-sharing-button resp-sharing-button--facebook resp-sharing-button--small\u0026#34;\u0026gt;\u0026lt;div aria-hidden=\u0026#34;true\u0026#34; class=\u0026#34;resp-sharing-button__icon resp-sharing-button__icon--solid\u0026#34;\u0026gt; \u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M18.77 7.46H14.5v-1.9c0-.9.6-1.1 1-1.1h3V.5h-4.33C10.24.5 9.5 3.44 9.5 5.32v2.15h-3v4h3v12h5v-12h3.85l.42-4z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;!-- Sharingbutton LinkedIn --\u0026gt; \u0026lt;a class=\u0026#34;resp-sharing-button__link\u0026#34; href=\u0026#34;https://www.linkedin.com/shareArticle?mini=true\u0026amp;amp;url={{ .Permalink }}\u0026amp;amp;title={{ .Title }}\u0026amp;amp;summary={{ .Title }}\u0026amp;amp;source={{ .Permalink }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34; aria-label=\u0026#34;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--small\u0026#34;\u0026gt;\u0026lt;div aria-hidden=\u0026#34;true\u0026#34; class=\u0026#34;resp-sharing-button__icon resp-sharing-button__icon--solid\u0026#34;\u0026gt; \u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M6.5 21.5h-5v-13h5v13zM4 6.5C2.5 6.5 1.5 5.3 1.5 4s1-2.4 2.5-2.4c1.6 0 2.5 1 2.6 2.5 0 1.4-1 2.5-2.6 2.5zm11.5 6c-1 0-2 1-2 2v7h-5v-13h5V10s1.6-1.5 4-1.5c3 0 5 2.2 5 6.3v6.7h-5v-7c0-1-1-2-2-2z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;!-- Sharingbutton Reddit --\u0026gt; \u0026lt;a class=\u0026#34;resp-sharing-button__link\u0026#34; href=\u0026#34;https://reddit.com/submit/?url={{ .Permalink }}\u0026amp;amp;resubmit=true\u0026amp;amp;title={{ .Title }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34; aria-label=\u0026#34;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;resp-sharing-button resp-sharing-button--reddit resp-sharing-button--small\u0026#34;\u0026gt;\u0026lt;div aria-hidden=\u0026#34;true\u0026#34; class=\u0026#34;resp-sharing-button__icon resp-sharing-button__icon--solid\u0026#34;\u0026gt; \u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M24 11.5c0-1.65-1.35-3-3-3-.96 0-1.86.48-2.42 1.24-1.64-1-3.75-1.64-6.07-1.72.08-1.1.4-3.05 1.52-3.7.72-.4 1.73-.24 3 .5C17.2 6.3 18.46 7.5 20 7.5c1.65 0 3-1.35 3-3s-1.35-3-3-3c-1.38 0-2.54.94-2.88 2.22-1.43-.72-2.64-.8-3.6-.25-1.64.94-1.95 3.47-2 4.55-2.33.08-4.45.7-6.1 1.72C4.86 8.98 3.96 8.5 3 8.5c-1.65 0-3 1.35-3 3 0 1.32.84 2.44 2.05 2.84-.03.22-.05.44-.05.66 0 3.86 4.5 7 10 7s10-3.14 10-7c0-.22-.02-.44-.05-.66 1.2-.4 2.05-1.54 2.05-2.84zM2.3 13.37C1.5 13.07 1 12.35 1 11.5c0-1.1.9-2 2-2 .64 0 1.22.32 1.6.82-1.1.85-1.92 1.9-2.3 3.05zm3.7.13c0-1.1.9-2 2-2s2 .9 2 2-.9 2-2 2-2-.9-2-2zm9.8 4.8c-1.08.63-2.42.96-3.8.96-1.4 0-2.74-.34-3.8-.95-.24-.13-.32-.44-.2-.68.15-.24.46-.32.7-.18 1.83 1.06 4.76 1.06 6.6 0 .23-.13.53-.05.67.2.14.23.06.54-.18.67zm.2-2.8c-1.1 0-2-.9-2-2s.9-2 2-2 2 .9 2 2-.9 2-2 2zm5.7-2.13c-.38-1.16-1.2-2.2-2.3-3.05.38-.5.97-.82 1.6-.82 1.1 0 2 .9 2 2 0 .84-.53 1.57-1.3 1.87z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;!-- Sharingbutton WhatsApp --\u0026gt; \u0026lt;a class=\u0026#34;resp-sharing-button__link\u0026#34; href=\u0026#34;whatsapp://send?text={{ .Title }}%20{{ .Permalink }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34; aria-label=\u0026#34;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;resp-sharing-button resp-sharing-button--whatsapp resp-sharing-button--small\u0026#34;\u0026gt;\u0026lt;div aria-hidden=\u0026#34;true\u0026#34; class=\u0026#34;resp-sharing-button__icon resp-sharing-button__icon--solid\u0026#34;\u0026gt; \u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M20.1 3.9C17.9 1.7 15 .5 12 .5 5.8.5.7 5.6.7 11.9c0 2 .5 3.9 1.5 5.6L.6 23.4l6-1.6c1.6.9 3.5 1.3 5.4 1.3 6.3 0 11.4-5.1 11.4-11.4-.1-2.8-1.2-5.7-3.3-7.8zM12 21.4c-1.7 0-3.3-.5-4.8-1.3l-.4-.2-3.5 1 1-3.4L4 17c-1-1.5-1.4-3.2-1.4-5.1 0-5.2 4.2-9.4 9.4-9.4 2.5 0 4.9 1 6.7 2.8 1.8 1.8 2.8 4.2 2.8 6.7-.1 5.2-4.3 9.4-9.5 9.4zm5.1-7.1c-.3-.1-1.7-.9-1.9-1-.3-.1-.5-.1-.7.1-.2.3-.8 1-.9 1.1-.2.2-.3.2-.6.1s-1.2-.5-2.3-1.4c-.9-.8-1.4-1.7-1.6-2-.2-.3 0-.5.1-.6s.3-.3.4-.5c.2-.1.3-.3.4-.5.1-.2 0-.4 0-.5C10 9 9.3 7.6 9 7c-.1-.4-.4-.3-.5-.3h-.6s-.4.1-.7.3c-.3.3-1 1-1 2.4s1 2.8 1.1 3c.1.2 2 3.1 4.9 4.3.7.3 1.2.5 1.6.6.7.2 1.3.2 1.8.1.6-.1 1.7-.7 1.9-1.3.2-.7.2-1.2.2-1.3-.1-.3-.3-.4-.6-.5z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/section\u0026gt; layouts/partials/article/components/footer.html In the end, add the partial template to footer.html for article pages.\n// ~~ \u0026lt;!-- Social media share buttons --\u0026gt; {{ partial \u0026#34;article/components/share-buttons\u0026#34; . }} \u0026lt;/footer\u0026gt; Now we have social media share buttons on the footer of articles! They are displayed below, right?\n","date":"2023-01-14T12:04:42+09:00","image":"https://nishipy.github.io/p/social-media-share-buttons-for-hugo-theme-stack/share-buttons_hu81c5837a67928c7bc03997a4103c9301_18652_120x120_fill_q75_box_smart1.jpg","permalink":"https://nishipy.github.io/p/social-media-share-buttons-for-hugo-theme-stack/","title":"Social Media Share Buttons for Hugo Theme Stack"},{"content":"見逃さない人 おそろしく速い手刀、オレでなきゃ見逃しちゃうね\nA person who does not miss that Such terrifying speed. No one except me could have seen that. ","date":"2023-01-09T12:47:43+09:00","permalink":"https://nishipy.github.io/p/hello-world/","title":"Hello World"}]