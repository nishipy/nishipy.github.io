[{"content":"Header Photo by Amy Asher on Unsplash\n🦭 はじめに Red Hat Advent Calendar 2023 17日目の記事🎅です。 ちなみにすべて 個人の見解 です。\n🦭 podman generate systemd の非推奨化と Quadlet エッジ等のユースケースで、systemdのサービスユニットとしてPodmanコンテナやPodを制御したい場合、従来は podman generate systemd というコマンドを使って、既存のコンテナやPodからユニットファイルを生成していました。2023年9月に発売された書籍Podmanイン・アクション(Podman in ActionをFedora 38/Podman v4.5.1を前提に日本語訳したもの) の「7章 systemdとの統合」でも、この方法を紹介しています。\nPodman v4.7.0 で、この `podman generate systemd はdeprecated(非推奨)となりました。systemdのもとでPodmanコンテナやPodを実行したい場合は、Podman v4.7.0 のリリースノートに記載がある通り、Quadlet を使うことが推奨されています。以下の記載があります。\nThe podman generate systemd command is deprecated. Use Quadlet for running containers and pods under systemd.\n🦭 Quadletを完全に理解する QuadletとPodman Quadletは、PodmanコンテナやPodをsystemdのもとでいい感じに実行できるようにするためのツールであり、もともとPodmanとは別のリポジトリ containers/quadlet で開発されていました。その後Podman v4.4で containers/podman 内の pkg/systemd の下にマージされています。Podman v4.4.0 のリリースノートには以下の記載があります。\nIntroduce Quadlet, a new systemd-generator that easily writes and maintains systemd services using Podman.\nQuadletはsystemdのgenerator そもそもQuadletとは何なんでしょうか。Quadletの作者であるAlexander LarssonさんのブログQuadlet, an easier way to run system containersには、以下のように書かれています。また、podman-systemd.unit のドキュメントもあわせて参照してださい。\nQuadlet is a systemd generator that takes a container description and automatically generates a systemd service file from it.\nつまるところ、Quadletはsystemdのgeneratorの１つです。generatorについては、systemd.generator(7)のman pageに詳しく説明されています。ChatGPTくんに頼んで要約してもらいましょう。\n(ChatGPTくん🤖): ジェネレータは、systemdの一部で、システムの起動時や設定再読み込み時に実行され、ユニットファイルがロードされる前に動的にユニットファイルやシンボリックリンクを生成する役割を持つ。ジェネレータは、特定のディレクトリに配置され、システム設定を拡張・上書きし、ユニットファイルの階層を拡張する。優先度の違うディレクトリに出力し、systemctl daemon-reloadで前の設定を消去し、ジェネレータを再実行してsystemdがユニットを再読み込む。\nわかった気になれたところで、man pageのExamplesを見てみましょう。ここでは最も馴染み深いであろう Example 1. systemd-fstab-generator を取り上げます。Quadletと同じくgeneratorの1つである systemd-fstab-generator が、/etc/fstab の内容をもとにユニットファイルを生成してくれています。\nExample 1. systemd-fstab-generator\nsystemd-fstab-generator(8) converts /etc/fstab into native mount units. It uses argv[1] as location to place the generated unit files in order to allow the user to override /etc/fstab with their own native unit files, but also to ensure that /etc/fstab overrides any vendor default from /usr/.\nAfter editing /etc/fstab, the user should invoke systemctl daemon-reload. This will re-run all generators and cause systemd to reload units from disk. To actually mount new directories added to fstab, systemctl start /path/to/mountpoint or systemctl start local-fs.target may be used.\nQuadletに話を戻しましょう。手元のFedora CoreOS 39な環境で、ユーザー用のgeneratorが置かれるディレクトリ /usr/lib/systemd/user-generators/ を覗いてみると、/usr/lib/systemd/user-generators/podman-user-generator というgeneratorがあります。これは /usr/libexec/podman/quadlet へのシンボリックリンクになっています。\ncore@fedora-39:~$ cat /etc/redhat-release Fedora release 39 (Thirty Nine) core@fedora-39:~$ podman version Client: Podman Engine Version: 4.7.2 API Version: 4.7.2 Go Version: go1.21.1 Built: Tue Oct 31 23:30:33 2023 OS/Arch: linux/arm64 ore@fedora-39:~$ ls -l /usr/lib/systemd/user-generators/podman-user-generator lrwxrwxrwx. 5 root root 31 Nov 21 04:19 /usr/lib/systemd/user-generators/podman-user-generator -\u0026gt; ../../../libexec/podman/quadlet /usr/libexec/podman/quadlet は実行可能なバイナリで、podman パッケージに含まれています。\ncore@fedora-39:~$ file /usr/libexec/podman/quadlet /usr/libexec/podman/quadlet: ELF 64-bit LSB pie executable, ARM aarch64, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-aarch64.so.1, BuildID[sha1]=59f30e7e5606e8c538765862854095113e30a9bb, for GNU/Linux 3.7.0, stripped core@fedora-39:~$ /usr/libexec/podman/quadlet -h Usage of /usr/libexec/podman/quadlet: -dryrun Run in dryrun mode printing debug information -no-kmsg-log Don\u0026#39;t log to kmsg -user Run as systemd user -v Print debug information -version Print version information and exit core@fedora-39:~$ rpm -qf /usr/libexec/podman/quadlet podman-4.7.2-1.fc39.aarch64 Quadlet ファイルの形式 Quadletがユニットファイルを生成するための元ファイル(以下、Quadletファイルと呼んでみます)の形式については、podman-systemd.unit(5)に書いてあります。\nQuadletファイルの置き場所は、rootユーザーとrootlessユーザーで異なります。以下のディレクトリのいずれかにおけばいいようです。\nrootユーザーの場合: /usr/share/containers/systemd/ /etc/containers/systemd/ rootlessユーザーの場合: $HOME/.config/containers/systemd/ $XDG_CONFIG_HOME/containers/systemd/ /etc/containers/systemd/users/$(UID) /etc/containers/systemd/users/ 例えば、Podmanコンテナを定義するためには、以下のような内容で.containerという拡張子のファイルを作成すればいいようです。systemdのユニットファイルに似た形式ですね。実際、[Service] と [Install] という部分はユニットファイルと同じように書けるようです。一方、[Container] はQuadlet特有ですね。他の例はpodman-systemd.unit(5)のExamplesを参照ください。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [Unit] Description=A minimal container [Container] # Use the centos image Image=quay.io/centos/centos:latest # Use volume and network defined below Volume=test.volume:/data Network=test.network # In the container we just run sleep Exec=sleep 60 [Service] # Restart service when sleep finishes Restart=always # Extend Timeout to allow time to pull the image TimeoutStartSec=900 # ExecStartPre flag and other systemd commands can go here, see systemd.unit(5) man page. ExecStartPre=/usr/share/mincontainer/setup.sh [Install] # Start by default on boot WantedBy=multi-user.target default.target Quadletでは [Container] 以外にも、[Kube] や [Volume]、[Network]、[Image] というセクションが定義できるようです。特にKubernetesのYAMLファイルを指定できる [Kube] は便利そうです。podman kube play のおかげですね。ちなみに、KubernetesとPodmanの連携についても、Podmanイン・アクションの「8章 Kubernetesとの連携」で紹介しています(宣伝)。\n🦭 Quadlet を使ってみる この章で使うQuadletファイルおよびKubernetes YAMLファイルは、このブログのGitHubリポジトリに置いています。\n.containerなQuadletファイルの使用 Podmanコンテナを実行するために、以下のQuadletファイル podman-hello.container を作成しました。oneshot で podman_hello_world を実行するだけです。rootlessユーザーを使うので、$HOME/.config/containers/systemd/ に置いています。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [Unit] Description=Hello World from Podman [Container] Image=quay.io/podman/hello:latest Exec=/usr/local/bin/podman_hello_world [Service] Restart=no Type=oneshot TimeoutStartSec=300 [Install] WantedBy=multi-user.target default.target このQuadletファイルからどのようなユニットファイルが生成されるかは、quadlet -dryrun -user で確認できます。\ncore@fedora-39:~$ /usr/libexec/podman/quadlet -dryrun -user quadlet-generator[21202]: Loading source unit file /var/home/core/.config/containers/systemd/podman-hello.container ---podman-hello.service--- [Unit] Description=Hello World from Podman SourcePath=/var/home/core/.config/containers/systemd/podman-hello.container RequiresMountsFor=%t/containers [X-Container] Image=quay.io/podman/hello:latest Exec=/usr/local/bin/podman_hello_world [Service] Restart=no Type=oneshot TimeoutStartSec=300 Environment=PODMAN_SYSTEMD_UNIT=%n KillMode=mixed ExecStop=/usr/bin/podman rm -f -i --cidfile=%t/%N.cid ExecStopPost=-/usr/bin/podman rm -f -i --cidfile=%t/%N.cid Delegate=yes SyslogIdentifier=%N ExecStart=/usr/bin/podman run --name=systemd-%N --cidfile=%t/%N.cid --replace --rm --cgroups=split quay.io/podman/hello:latest /usr/local/bin/podman_hello_world [Install] WantedBy=multi-user.target default.target .container なQuadletファイルからユニットファイルを生成するロジックはこの辺に実装されています。とても長いですが、Quadletファイルの [Container] の下に書かれたパラメータをひたすら変換して、ExecStart=/usr/bin/podman run の引数に追加していってるのがわかるかと思います。\nさて、Quadletファイルを書いた後 systemctl --user daemon-reload すると、podman-hello.service というユニットファイルを生成されます。内容は先程 quadlet -dryrun -user で確認したものと同じです。\ncore@fedora-39:~$ systemctl --user daemon-reload core@fedora-39:~$ file /run/user/$UID/systemd/generator/podman-hello.service /run/user/501/systemd/generator/podman-hello.service: ASCII text core@fedora-39:~$ file /run/user/$UID/systemd/generator/*/podman-hello.service /run/user/501/systemd/generator/default.target.wants/podman-hello.service: symbolic link to ../podman-hello.service /run/user/501/systemd/generator/multi-user.target.wants/podman-hello.service: symbolic link to ../podman-hello.service このsystemdサービスを起動すると、🦭が挨拶だけしてすぐ帰っていくはずです。\ncore@fedora-39:~$ systemctl --user start podman-hello.service core@fedora-39:~$ journalctl --user -u podman-hello.service Dec 11 19:06:34 fedora-39 podman[22326]: 2023-12-11 19:06:34.164871911 +0900 JST m=+0.116574772 container start 3c1197296f89b6f70afb20a6ac01fa53f41f7bca1875578bffb965749984ffac (image=quay.io/podman/hello:latest, name=syst\u0026gt; Dec 11 19:06:34 fedora-39 podman[22326]: 2023-12-11 19:06:34.165512076 +0900 JST m=+0.117214937 container attach 3c1197296f89b6f70afb20a6ac01fa53f41f7bca1875578bffb965749984ffac (image=quay.io/podman/hello:latest, name=sys\u0026gt; Dec 11 19:06:34 fedora-39 podman-hello[22326]: !... Hello Podman World ...! Dec 11 19:06:34 fedora-39 podman-hello[22326]: .--\u0026#34;--. Dec 11 19:06:34 fedora-39 podman-hello[22326]: / - - \\ Dec 11 19:06:34 fedora-39 podman-hello[22326]: / (O) (O) \\ Dec 11 19:06:34 fedora-39 podman-hello[22326]: ~~~| -=(,Y,)=- | Dec 11 19:06:34 fedora-39 podman-hello[22326]: .---. /` \\ |~~ Dec 11 19:06:34 fedora-39 podman-hello[22326]: ~/ o o \\~~~~.----. ~~ Dec 11 19:06:34 fedora-39 podman-hello[22326]: | =(X)= |~ / (O (O) \\ Dec 11 19:06:34 fedora-39 podman-hello[22326]: ~~~~~~~ ~| =(Y_)=- | Dec 11 19:06:34 fedora-39 podman-hello[22326]: ~~~~ ~~~| U |~~ Dec 11 19:06:34 fedora-39 podman-hello[22326]: Project: https://github.com/containers/podman Dec 11 19:06:34 fedora-39 podman-hello[22326]: Website: https://podman.io Dec 11 19:06:34 fedora-39 podman-hello[22326]: Documents: https://docs.podman.io Dec 11 19:06:34 fedora-39 podman-hello[22326]: Twitter: @Podman_io Dec 11 19:06:34 fedora-39 podman[22326]: 2023-12-11 19:06:34.1658957 +0900 JST m=+0.117598561 container died 3c1197296f89b6f70afb20a6ac01fa53f41f7bca1875578bffb965749984ffac (image=quay.io/podman/hello:latest, name=systemd\u0026gt; Dec 11 19:06:34 fedora-39 podman[22399]: 2023-12-11 19:06:34.23270892 +0900 JST m=+0.060999942 container remove 3c1197296f89b6f70afb20a6ac01fa53f41f7bca1875578bffb965749984ffac (image=quay.io/podman/hello:latest, name=syst\u0026gt; Dec 11 19:06:34 fedora-39 systemd[985]: Finished podman-hello.service - Hello World from Podman. .kubeなQuadletファイルの使用 他のQuadletファイルも試してみましょう。.kube という拡張子のQuadletファイルに [Kube] セクションを設定すると、podman kube play を実行するユニットファイルが生成されます。\npodman kube play では、KubernetesなYAMLファイルからPodmanコンテナやPod、Volumeなどを作成することができます。執筆時点で最新のPodman v4.8.0では、以下のKubernetesリソースがサポートされています。\nPod Deployment PersistentVolumeClaim ConfigMap Secret DaemonSet ということで、まずはKubernetes YAMLの準備です。今回は Pod と ConfigMap を使いたいので、Kubernetesのドキュメント Configure a Pod to Use a ConfigMap に載っている例を拝借します。\nConfigMapのマニフェストはこれで、\n1 2 3 4 5 6 7 8 apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: special.how: very special.type: charm Podはこれです。very charmなPodができそうな予感。\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: registry.k8s.io/busybox command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;env\u0026#34; ] envFrom: - configMapRef: name: special-config restartPolicy: Never 準備ができたところで、.kubeなQuadletファイルを書きましょう。[Kube]のところは、↑のYAMLのパスをそのまま指定すればいいだけでとても楽です。ConfigMapが複数ある場合には、ConfigMap=の行を必要な分だけ足していけばいいらしいです。\n1 2 3 4 5 6 7 8 9 10 11 12 13 [Unit] Description=A Service of very charm Pod [Kube] Yaml=/tmp/quadlet-in-action/pod-configmap-envFrom.yaml ConfigMap=/tmp/quadlet-in-action/special-config.yaml [Service] Restart=no TimeoutStartSec=300 [Install] WantedBy=multi-user.target default.target こんなので動くのか不安なので quadlet -dryrun -user してみると、ちゃんとユニットファイル変換されました。.container のときと違って、ExecStart には podman kube play が指定されています。\ncore@fedora-39:~$ /usr/libexec/podman/quadlet -dryrun -user ~/.config/containers/systemd/special-pod.kube ...snip... quadlet-generator[42819]: Loading source unit file /var/home/core/.config/containers/systemd/special-pod.kube ...snip... ---special-pod.service--- [Unit] Description=A Service of very charm Pod SourcePath=/var/home/core/.config/containers/systemd/special-pod.kube RequiresMountsFor=%t/containers [X-Kube] Yaml=/tmp/quadlet-in-action/pod-configmap-envFrom.yaml ConfigMap=/tmp/quadlet-in-action/special-config.yaml [Service] Restart=no Type=notify TimeoutStartSec=300 KillMode=mixed Environment=PODMAN_SYSTEMD_UNIT=%n NotifyAccess=all SyslogIdentifier=%N ExecStart=/usr/bin/podman kube play --replace --service-container=true --configmap /tmp/quadlet-in-action/special-config.yaml /tmp/quadlet-in-action/pod-configmap-envFrom.yaml ExecStopPost=/usr/bin/podman kube down /tmp/quadlet-in-action/pod-configmap-envFrom.yaml [Install] WantedBy=multi-user.target default.target 忘れずに systemctl --user daemon-reload した後、special-pod.service を起動してみましょう。very charmなコンテナがsystemdサービスとして実行されたことがわかります。\ncore@fedora-39:~$ systemctl --user daemon-reload core@fedora-39:~$ systemctl --user start special-pod.service core@fedora-39:~$ journalctl --user -u special-pod.service --grep \u0026#34;special\\.\u0026#34; Dec 12 01:59:06 fedora-39 dapi-test-pod-test-container[327907]: special.type=charm Dec 12 01:59:06 fedora-39 dapi-test-pod-test-container[327907]: special.how=very 🦭 Quadlet は podman-generate-systemd の代替になるのか？ 個人的な意見として、podman-generate-systemd に慣れ親しんだユーザーは、Quadlet が代替と言われるとモヤると思います。というのも、podman-generate-systemd では既存のコンテナやPodからユニットファイルを生成することができました。一方、Quadlet ファイルは自分で書く必要があり、podman-generate-systemd のように既存のコンテナやPodからQuadlet ファイルを自動生成する機能はありません。\nこの辺りは、containers/podman の Discussion#20218の通りコミュニティでも議論されています。How to replace the \u0026quot;podman generate systemd\u0026quot; command since its deprecated という質問に対して、PodmanのMaintainerが投稿した以下のコメントがベストアンサーに選ばれています。\nI do not want to add that functionality to Podman as the ultimately goal is to move away from the workflow of generating systemd units based on existing containers. With Quadlet we aim at a Compose and K8s-like declarative workflow. Adding podman generate quadlet conflicts with that goal and it would imply that new Quadlet features had to be added there as well. The current strategy is to deprecate podman generate systemd. That means that no new features will be added to generate systemd but only bug fixes.\nこのコメントから分かるように、Podmanは最終的には既存のコンテナからユニットファイルを生成する既存のワークフローから脱却して、KubernetesやComposeのような宣言的なワークフローを目指しており、これがPodmanにQuadletをマージした目的とのこと。このゴールと衝突するため、Quadletなファイルを生成する機能をPodmanに追加する予定はないようです。\n確かにQuadletはコンテナやPodだけでなく、NetworkやVolume、Imageも宣言的に書くことができ再配布しやすいですね。Kubernetes YAMLを使ったPodmanコンテナやPodの起動をsystemdと連携できるのも魅力です。\nちなみに、podman run コマンドなどからQuadletファイルを生成する podlet というツールは開発されているようです。\n🦭 おわりに 以上で、Podmanとsystemdを連携させるQuadletを完全に理解できました。Podmanユーザーの方はもちろん、Dockerユーザーの方も興味あればぜひ触ってみてください。WindowsやMacをお使いでも、Podman Desktopで簡単に試せると思います。Quadletで遊びたい場合は、systemdがinitプロセスである必要があるので、podman machine init して Fedora CoreOS のVMを作成した後に、podman machine sshして試すのが簡単です。\n[nishipy-MBP] $ podman machine list NAME VM TYPE CREATED LAST UP CPUS MEMORY DISK SIZE fedoracos* qemu 4 days ago Currently running 1 2GiB 100GiB [nishipy-MBP] $ podman machine ssh fedoracos Connecting to vm fedoracos. To close connection, use `~.` or `exit` Fedora CoreOS 39.20231119.2.0 Tracker: https://github.com/coreos/fedora-coreos-tracker Discuss: https://discussion.fedoraproject.org/tag/coreos Last login: Fri Dec 11 20:12:59 2023 from 192.168.127.1 core@fedora-39:~$ 以上です。この記事のタイトルが怒られないことを祈りつつ\u0026hellip;🎅\n🦭 (余談)containers/ansible-podman-collections でも Quadlet をサポートしたい？ 最近Pythonを書きたくなった時にPR送っている Ansible Collection containers/ansible-podman-collections でも、Quadletをサポートするモジュール/オプションを追加したいな〜と思い、Issue作って提案してみました。Quadletファイル詳しい人は、ansible.builtin.template module使う気もしますが\u0026hellip;\nhttps://github.com/containers/ansible-podman-collections/issues/671\n🦭 References Quadlet, an easier way to run system containers | Alexander Larsson Make systemd better for Podman with Quadlet | Red Hat - Enable Sysadmin Deploying a multi-container application using Podman and Quadlet | Red Hat - Enable Sysadmin How to replace the \u0026ldquo;podman generate systemd\u0026rdquo; command since its deprecated #20218 - containers/podman | GitHub podman-generate-systemd(1) | Podman Docs podman-systemd.unit(5) | Podman Docs ","date":"2023-12-17T00:00:00+09:00","image":"https://blog.nishipy.com/p/podman-quadlet/seal_hu3d03a01dcc18bc5be0e67db3d8d209a6_555214_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.nishipy.com/p/podman-quadlet/","title":"Quadletイン・アクション"},{"content":"For the goal of this series or abstract of the PLEG and so on, please refer to the previous post Understanding PLEG with source code - Part 1. Now, let\u0026rsquo;s read the source code of PLEG! Note that the source code in the following part comes from Kubernetes 1.25.\npkg/kubelet/pleg/pleg.go The types of PodLifeCycleEvents in PLEG and their associated structures and interfaces are defined here. We have some events like ContainerStarted, ContainerDied, ContainerRemoved, PodSync and ContainerChanged.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // PodLifeCycleEventType define the event type of pod life cycle events. type PodLifeCycleEventType string const ( // ContainerStarted - event type when the new state of container is running. ContainerStarted PodLifeCycleEventType = \u0026#34;ContainerStarted\u0026#34; // ContainerDied - event type when the new state of container is exited. ContainerDied PodLifeCycleEventType = \u0026#34;ContainerDied\u0026#34; // ContainerRemoved - event type when the old state of container is exited. ContainerRemoved PodLifeCycleEventType = \u0026#34;ContainerRemoved\u0026#34; // PodSync is used to trigger syncing of a pod when the observed change of // the state of the pod cannot be captured by any single event above. PodSync PodLifeCycleEventType = \u0026#34;PodSync\u0026#34; // ContainerChanged - event type when the new state of container is unknown. ContainerChanged PodLifeCycleEventType = \u0026#34;ContainerChanged\u0026#34; ) // PodLifecycleEvent is an event that reflects the change of the pod state. type PodLifecycleEvent struct { // The pod ID. ID types.UID // The type of the event. Type PodLifeCycleEventType // The accompanied data which varies based on the event type. // - ContainerStarted/ContainerStopped: the container name (string). // - All other event types: unused. Data interface{} } // PodLifecycleEventGenerator contains functions for generating pod life cycle events. type PodLifecycleEventGenerator interface { Start() Watch() chan *PodLifecycleEvent Healthy() (bool, error) } pkg/kubelet/pleg/generic.go The PLEG-related source code is mainly written in this file. The GenericPLEG structure looks as follows. As for the Cache, please refer to the Runtime Pod Cache described in the previous article. Looking at the comments in kubecontainer.Cache interface, Cache seems to store the PodStatus of all containers/pods visible to the container runtime.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 type GenericPLEG struct { // The period for relisting. relistPeriod time.Duration // The container runtime. runtime kubecontainer.Runtime // The channel from which the subscriber listens events. eventChannel chan *PodLifecycleEvent // The internal cache for pod/container information. podRecords podRecords // Time of the last relisting. relistTime atomic.Value // Cache for storing the runtime states required for syncing pods. cache kubecontainer.Cache // For testability. clock clock.Clock // Pods that failed to have their status retrieved during a relist. These pods will be // retried during the next relisting. podsToReinspect map[types.UID]*kubecontainer.Pod } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // Cache stores the PodStatus for the pods. It represents *all* the visible // pods/containers in the container runtime. All cache entries are at least as // new or newer than the global timestamp (set by UpdateTime()), while // individual entries may be slightly newer than the global timestamp. If a pod // has no states known by the runtime, Cache returns an empty PodStatus object // with ID populated. // // Cache provides two methods to retrieve the PodStatus: the non-blocking Get() // and the blocking GetNewerThan() method. The component responsible for // populating the cache is expected to call Delete() to explicitly free the // cache entries. type Cache interface { Get(types.UID) (*PodStatus, error) Set(types.UID, *PodStatus, error, time.Time) // GetNewerThan is a blocking call that only returns the status // when it is newer than the given time. GetNewerThan(types.UID, time.Time) (*PodStatus, error) Delete(types.UID) UpdateTime(time.Time) } The constant definition looks like this: relistThreshold is 3 minutes. When the Kubernetes/OpenShift nodes get NotReady, I saw this threshold in the error message PLEG is not healthy: pleg was last seen active XXmYYs ago; threshold is 3m0s.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // plegContainerState has a one-to-one mapping to the // kubecontainer.State except for the non-existent state. This state // is introduced here to complete the state transition scenarios. type plegContainerState string const ( plegContainerRunning plegContainerState = \u0026#34;running\u0026#34; plegContainerExited plegContainerState = \u0026#34;exited\u0026#34; plegContainerUnknown plegContainerState = \u0026#34;unknown\u0026#34; plegContainerNonExistent plegContainerState = \u0026#34;non-existent\u0026#34; // The threshold needs to be greater than the relisting period + the // relisting time, which can vary significantly. Set a conservative // threshold to avoid flipping between healthy and unhealthy. relistThreshold = 3 * time.Minute ) *GenericPLEG.Healthy() The Healthy() method is a pointer method of the GenericPLEG structure. It checks if the PLEG itself is working properly. More specifically, if the interval between two relists exceeds 3 minutes (if elapsed \u0026gt; relistThreshold), the PLEG get failed. Then it reports the errors like PLEG is not healthy: pleg was last seen active XXmYYs ago; threshold is 3m0s.\nIt seems that elapsed will measure how much time has elapsed since the relistTime, which is a time of type time.Time and shows the time when the last relist was started).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // Healthy check if PLEG work properly. // relistThreshold is the maximum interval between two relist. func (g *GenericPLEG) Healthy() (bool, error) { relistTime := g.getRelistTime() if relistTime.IsZero() { return false, fmt.Errorf(\u0026#34;pleg has yet to be successful\u0026#34;) } // Expose as metric so you can alert on `time()-pleg_last_seen_seconds \u0026gt; nn` metrics.PLEGLastSeen.Set(float64(relistTime.Unix())) elapsed := g.clock.Since(relistTime) if elapsed \u0026gt; relistThreshold { return false, fmt.Errorf(\u0026#34;pleg was last seen active %v ago; threshold is %v\u0026#34;, elapsed, relistThreshold) } return true, nil } // ~~ func (g *GenericPLEG) getRelistTime() time.Time { val := g.relistTime.Load() if val == nil { return time.Time{} } return val.(time.Time) } *GenericPLEG.relist() PLEG\u0026rsquo;s relisting is implemented in the relist() method. relist() queries the container runtime to obtain a list of pods/containers. It then compares it to the internal pods/containers and generates events accordingly. relist() performs the following steps:\nFirst, the current time is retrieved. After retrieving all pod information from the container runtime, relistTime is updated as the timestamp of the most recent start of relist(). g.runtime.GetPods(true) is passed with an argument true to retrieve the list of containers including exited and dead ones. The retrieved list is used to update the data in podRecords (internal cache of pod/container information).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // relist queries the container runtime for list of pods/containers, compare // with the internal pods/containers, and generates events accordingly. func (g *GenericPLEG) relist() { klog.V(5).InfoS(\u0026#34;GenericPLEG: Relisting\u0026#34;) if lastRelistTime := g.getRelistTime(); !lastRelistTime.IsZero() { metrics.PLEGRelistInterval.Observe(metrics.SinceInSeconds(lastRelistTime)) } timestamp := g.clock.Now() defer func() { metrics.PLEGRelistDuration.Observe(metrics.SinceInSeconds(timestamp)) }() // Get all the pods. podList, err := g.runtime.GetPods(true) if err != nil { klog.ErrorS(err, \u0026#34;GenericPLEG: Unable to retrieve pods\u0026#34;) return } g.updateRelistTime(timestamp) pods := kubecontainer.Pods(podList) // update running pod and container count updateRunningPodAndContainerMetrics(pods) g.podRecords.setCurrent(pods) // ~~ PodRecord has the old and current fields, and the type of both is type kubecontainer.Pod. kubecontainer.Pod structure looks like as follows and contains information such as Pod name, Namespace, and list of containers.\n1 2 3 4 5 6 type podRecord struct { old *kubecontainer.Pod current *kubecontainer.Pod } type podRecords map[types.UID]*podRecord 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Pod is a group of containers. type Pod struct { // The ID of the pod, which can be used to retrieve a particular pod // from the pod list returned by GetPods(). ID types.UID // The name and namespace of the pod, which is readable by human. Name string Namespace string // List of containers that belongs to this pod. It may contain only // running containers, or mixed with dead ones (when GetPods(true)). Containers []*Container // List of sandboxes associated with this pod. The sandboxes are converted // to Container temporarily to avoid substantial changes to other // components. This is only populated by kuberuntime. // TODO: use the runtimeApi.PodSandbox type directly. Sandboxes []*Container } Now, let\u0026rsquo;s go back to the relist. For each pod, the current (current) and past (old) status of podRecords will be compared and events will be generated accordingly. The generated events are added to Map eventsByPodID. The pods are walked through one by one from this Map eventsByPodID, and if there is an event, then the PodCache will be updated.\nFrom case g.eventChannel \u0026lt;- events[i], we can see that the event will be sent if the PodLyifecycleEvent channel has room.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 func (g *GenericPLEG) relist() { // ~~ // Compare the old and the current pods, and generate events. eventsByPodID := map[types.UID][]*PodLifecycleEvent{} for pid := range g.podRecords { oldPod := g.podRecords.getOld(pid) pod := g.podRecords.getCurrent(pid) // Get all containers in the old and the new pod. allContainers := getContainersFromPods(oldPod, pod) for _, container := range allContainers { events := computeEvents(oldPod, pod, \u0026amp;container.ID) for _, e := range events { updateEvents(eventsByPodID, e) } } } var needsReinspection map[types.UID]*kubecontainer.Pod if g.cacheEnabled() { needsReinspection = make(map[types.UID]*kubecontainer.Pod) } // If there are events associated with a pod, we should update the // podCache. for pid, events := range eventsByPodID { pod := g.podRecords.getCurrent(pid) if g.cacheEnabled() { // updateCache() will inspect the pod and update the cache. If an // error occurs during the inspection, we want PLEG to retry again // in the next relist. To achieve this, we do not update the // associated podRecord of the pod, so that the change will be // detect again in the next relist. // TODO: If many pods changed during the same relist period, // inspecting the pod and getting the PodStatus to update the cache // serially may take a while. We should be aware of this and // parallelize if needed. if err := g.updateCache(pod, pid); err != nil { // Rely on updateCache calling GetPodStatus to log the actual error. klog.V(4).ErrorS(err, \u0026#34;PLEG: Ignoring events for pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KRef(pod.Namespace, pod.Name)) // make sure we try to reinspect the pod during the next relisting needsReinspection[pid] = pod continue } else { // this pod was in the list to reinspect and we did so because it had events, so remove it // from the list (we don\u0026#39;t want the reinspection code below to inspect it a second time in // this relist execution) delete(g.podsToReinspect, pid) } } // Update the internal storage and send out the events. g.podRecords.update(pid) // Map from containerId to exit code; used as a temporary cache for lookup containerExitCode := make(map[string]int) func (g *GenericPLEG) relist() { // ~~ // Compare the old and the current pods, and generate events. eventsByPodID := map[types.UID][]*PodLifecycleEvent{} for pid := range g.podRecords { oldPod := g.podRecords.getOld(pid) pod := g.podRecords.getCurrent(pid) // Get all containers in the old and the new pod. allContainers := getContainersFromPods(oldPod, pod) for _, container := range allContainers { events := computeEvents(oldPod, pod, \u0026amp;container.ID) for _, e := range events { updateEvents(eventsByPodID, e) } } } var needsReinspection map[types.UID]*kubecontainer.Pod if g.cacheEnabled() { needsReinspection = make(map[types.UID]*kubecontainer.Pod) } // If there are events associated with a pod, we should update the // podCache. for pid, events := range eventsByPodID { pod := g.podRecords.getCurrent(pid) if g.cacheEnabled() { // updateCache() will inspect the pod and update the cache. If an // error occurs during the inspection, we want PLEG to retry again // in the next relist. To achieve this, we do not update the // associated podRecord of the pod, so that the change will be // detect again in the next relist. // TODO: If many pods changed during the same relist period, // inspecting the pod and getting the PodStatus to update the cache // serially may take a while. We should be aware of this and // parallelize if needed. if err := g.updateCache(pod, pid); err != nil { // Rely on updateCache calling GetPodStatus to log the actual error. klog.V(4).ErrorS(err, \u0026#34;PLEG: Ignoring events for pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KRef(pod.Namespace, pod.Name)) // make sure we try to reinspect the pod during the next relisting needsReinspection[pid] = pod continue } else { // this pod was in the list to reinspect and we did so because it had events, so remove it // from the list (we don\u0026#39;t want the reinspection code below to inspect it a second time in // this relist execution) delete(g.podsToReinspect, pid) } } // Update the internal storage and send out the events. g.podRecords.update(pid) // Map from containerId to exit code; used as a temporary cache for lookup containerExitCode := make(map[string]int) for i := range events { // Filter out events that are not reliable and no other components use yet. if events[i].Type == ContainerChanged { continue } select { case g.eventChannel \u0026lt;- events[i]: default: metrics.PLEGDiscardEvents.Inc() klog.ErrorS(nil, \u0026#34;Event channel is full, discard this relist() cycle event\u0026#34;) } // Log exit code of containers when they finished in a particular event if events[i].Type == ContainerDied { // Fill up containerExitCode map for ContainerDied event when first time appeared if len(containerExitCode) == 0 \u0026amp;\u0026amp; pod != nil \u0026amp;\u0026amp; g.cache != nil { // Get updated podStatus status, err := g.cache.Get(pod.ID) if err == nil { for _, containerStatus := range status.ContainerStatuses { containerExitCode[containerStatus.ID.ID] = containerStatus.ExitCode } } } if containerID, ok := events[i].Data.(string); ok { if exitCode, ok := containerExitCode[containerID]; ok \u0026amp;\u0026amp; pod != nil { klog.V(2).InfoS(\u0026#34;Generic (PLEG): container finished\u0026#34;, \u0026#34;podID\u0026#34;, pod.ID, \u0026#34;containerID\u0026#34;, containerID, \u0026#34;exitCode\u0026#34;, exitCode) } } } } } if g.cacheEnabled() { // reinspect any pods that failed inspection during the previous relist if len(g.podsToReinspect) \u0026gt; 0 { klog.V(5).InfoS(\u0026#34;GenericPLEG: Reinspecting pods that previously failed inspection\u0026#34;) for pid, pod := range g.podsToReinspect { if err := g.updateCache(pod, pid); err != nil { // Rely on updateCache calling GetPodStatus to log the actual error. klog.V(5).ErrorS(err, \u0026#34;PLEG: pod failed reinspection\u0026#34;, \u0026#34;pod\u0026#34;, klog.KRef(pod.Namespace, pod.Name)) needsReinspection[pid] = pod } } } // Update the cache timestamp. This needs to happen *after* // all pods have been properly updated in the cache. g.cache.UpdateTime(timestamp) } // make sure we retain the list of pods that need reinspecting the next time relist is called g.podsToReinspect = needsReinspection } Pod Lifecycle Event generation is performed by generateEvents() below. As you can see from the first if statement, if the state is the same as the previous state, no event will be generated.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func generateEvents(podID types.UID, cid string, oldState, newState plegContainerState) []*PodLifecycleEvent { if newState == oldState { return nil } klog.V(4).InfoS(\u0026#34;GenericPLEG\u0026#34;, \u0026#34;podUID\u0026#34;, podID, \u0026#34;containerID\u0026#34;, cid, \u0026#34;oldState\u0026#34;, oldState, \u0026#34;newState\u0026#34;, newState) switch newState { case plegContainerRunning: return []*PodLifecycleEvent{{ID: podID, Type: ContainerStarted, Data: cid}} case plegContainerExited: return []*PodLifecycleEvent{{ID: podID, Type: ContainerDied, Data: cid}} case plegContainerUnknown: return []*PodLifecycleEvent{{ID: podID, Type: ContainerChanged, Data: cid}} case plegContainerNonExistent: switch oldState { case plegContainerExited: // We already reported that the container died before. return []*PodLifecycleEvent{{ID: podID, Type: ContainerRemoved, Data: cid}} default: return []*PodLifecycleEvent{{ID: podID, Type: ContainerDied, Data: cid}, {ID: podID, Type: ContainerRemoved, Data: cid}} } default: panic(fmt.Sprintf(\u0026#34;unrecognized container state: %v\u0026#34;, newState)) } } *GenericPLEG.Start() In the Start() method, the relist() method will be executed in goroutine. The wait.Until() is used here, hence it loops at relistPeriod (= 1 second) intervals.\n1 2 3 4 // Start spawns a goroutine to relist periodically. func (g *GenericPLEG) Start() { go wait.Until(g.relist, g.relistPeriod, wait.NeverStop) } *GenericPLEG.Watch() Watch() is a method that returns a channel for PodLifecycleEvents. It returns the channel *PodLifecycleEvent, and the kubelet receives events from this channel and performs the Pod synchronizations as appropriate.\n1 2 3 4 5 6 // Watch returns a channel from which the subscriber can receive PodLifecycleEvent // events. // TODO: support multiple subscribers. func (g *GenericPLEG) Watch() chan *PodLifecycleEvent { return g.eventChannel } pkg/kubelet/kubelet.go Furthermore, let\u0026rsquo;s see how PLEG is called on the kubelet side. There are some PLEG-related constants: the relist interval is 1 second, and the capacity of the channel for PodLifecycleEvents looks 1000.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 const ( //~~ // Capacity of the channel for receiving pod lifecycle events. This number // is a bit arbitrary and may be adjusted in the future. plegChannelCapacity = 1000 // Generic PLEG relies on relisting for discovering container events. // A longer period means that kubelet will take longer to detect container // changes and to update pod status. On the other hand, a shorter period // will cause more frequent relisting (e.g., container runtime operations), // leading to higher cpu usage. // Note that even though we set the period to 1s, the relisting itself can // take more than 1s to finish if the container runtime responds slowly // and/or when there are many container changes in one cycle. plegRelistPeriod = time.Second * 1 //~~ ) The NewMainKubelet() will instantiate a new Kubelet object along with all necessary internal modules. The PLEG-related processing is as follows. After instantiating the GenericPLEG object in NewGenericPLEG(), it will be added to the health check mechanism in the main loop of the kubelet. As a result, we sometimes see the error messages like the PLEG is not healthy: pleg was last seen active XXmYYs ago; threshold is 3m0s in the kubelet log.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // NewMainKubelet instantiates a new Kubelet object along with all the required internal modules. // No initialization of Kubelet and its modules should happen here. func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, crOptions *config.ContainerRuntimeOptions, hostname string, hostnameOverridden bool, // ~~ seccompDefault bool, ) (*Kubelet, error) { // ~~ klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, klet.podCache, clock.RealClock{}) klet.runtimeState = newRuntimeState(maxWaitForContainerRuntime) klet.runtimeState.addHealthCheck(\u0026#34;PLEG\u0026#34;, klet.pleg.Healthy) // ~~ } You can see that PLEG is also started with kl.pleg.Start() in the Run() method on the kubelet side.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 // Run starts the kubelet reacting to config updates func (kl *Kubelet) Run(updates \u0026lt;-chan kubetypes.PodUpdate) { if kl.logServer == nil { kl.logServer = http.StripPrefix(\u0026#34;/logs/\u0026#34;, http.FileServer(http.Dir(\u0026#34;/var/log/\u0026#34;))) } if kl.kubeClient == nil { klog.InfoS(\u0026#34;No API server defined - no node status update will be sent\u0026#34;) } // Start the cloud provider sync manager if kl.cloudResourceSyncManager != nil { go kl.cloudResourceSyncManager.Run(wait.NeverStop) } if err := kl.initializeModules(); err != nil { kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error()) klog.ErrorS(err, \u0026#34;Failed to initialize internal modules\u0026#34;) os.Exit(1) } // Start volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) if kl.kubeClient != nil { // Introduce some small jittering to ensure that over time the requests won\u0026#39;t start // accumulating at approximately the same time from the set of nodes due to priority and // fairness effect. go wait.JitterUntil(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, 0.04, true, wait.NeverStop) go kl.fastStatusUpdateOnce() // start syncing lease go kl.nodeLeaseController.Run(wait.NeverStop) } go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) // Set up iptables util rules if kl.makeIPTablesUtilChains { kl.initNetworkUtil() } // Start component sync loops. kl.statusManager.Start() // Start syncing RuntimeClasses if enabled. if kl.runtimeClassManager != nil { kl.runtimeClassManager.Start(wait.NeverStop) } // Start the pod lifecycle event generator. kl.pleg.Start() kl.syncLoop(updates, kl) } At the end of the Run() method, the syncLoop() method is also called. The plegCh := kl.pleg.Watch() part in syncLoop() obtains a channel to read PLEG updates and receives events generated by PLEG via the channel.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 // syncLoop is the main loop for processing changes. It watches for changes from // three channels (file, apiserver, and http) and creates a union of them. For // any new change seen, will run a sync against desired state and running state. If // no changes are seen to the configuration, will synchronize the last known desired // state every sync-frequency seconds. Never returns. func (kl *Kubelet) syncLoop(updates \u0026lt;-chan kubetypes.PodUpdate, handler SyncHandler) { klog.InfoS(\u0026#34;Starting kubelet main sync loop\u0026#34;) // The syncTicker wakes up kubelet to checks if there are any pod workers // that need to be sync\u0026#39;d. A one-second period is sufficient because the // sync interval is defaulted to 10s. syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) duration := base // Responsible for checking limits in resolv.conf // The limits do not have anything to do with individual pods // Since this is called in syncLoop, we don\u0026#39;t need to call it anywhere else if kl.dnsConfigurer != nil \u0026amp;\u0026amp; kl.dnsConfigurer.ResolverConfig != \u0026#34;\u0026#34; { kl.dnsConfigurer.CheckLimitsForResolvConf() } for { if err := kl.runtimeState.runtimeErrors(); err != nil { klog.ErrorS(err, \u0026#34;Skipping pod synchronization\u0026#34;) // exponential backoff time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue } // reset backoff if we have a success duration = base kl.syncLoopMonitor.Store(kl.clock.Now()) if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) { break } kl.syncLoopMonitor.Store(kl.clock.Now()) } } syncLoopIteration() is a method that reads various channels and dispatches them to a given Handler. The plegCh channel is described as being used for Runtime Cache updates and Pod synchronization. For example, if the kubelet receives the event PLEG\u0026rsquo;s ContainerDied (= the latest container state is Exited), the kubelet will delete the container instance in question in the pod via cleanUpContainersInPod().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 // syncLoopIteration reads from various channels and dispatches pods to the // given handler. // // Arguments: // 1. configCh: a channel to read config events from // 2. handler: the SyncHandler to dispatch pods to // 3. syncCh: a channel to read periodic sync events from // 4. housekeepingCh: a channel to read housekeeping events from // 5. plegCh: a channel to read PLEG updates from // // Events are also read from the kubelet liveness manager\u0026#39;s update channel. // // The workflow is to read from one of the channels, handle that event, and // update the timestamp in the sync loop monitor. // // Here is an appropriate place to note that despite the syntactical // similarity to the switch statement, the case statements in a select are // evaluated in a pseudorandom order if there are multiple channels ready to // read from when the select is evaluated. In other words, case statements // are evaluated in random order, and you can not assume that the case // statements evaluate in order if multiple channels have events. // // With that in mind, in truly no particular order, the different channels // are handled as follows: // // - configCh: dispatch the pods for the config change to the appropriate // handler callback for the event type // - plegCh: update the runtime cache; sync pod // - syncCh: sync all pods waiting for sync // - housekeepingCh: trigger cleanup of pods // - health manager: sync pods that have failed or in which one or more // containers have failed health checks func (kl *Kubelet) syncLoopIteration(configCh \u0026lt;-chan kubetypes.PodUpdate, handler SyncHandler, syncCh \u0026lt;-chan time.Time, housekeepingCh \u0026lt;-chan time.Time, plegCh \u0026lt;-chan *pleg.PodLifecycleEvent) bool { // ~~ case e := \u0026lt;-plegCh: if e.Type == pleg.ContainerStarted { // record the most recent time we observed a container start for this pod. // this lets us selectively invalidate the runtimeCache when processing a delete for this pod // to make sure we don\u0026#39;t miss handling graceful termination for containers we reported as having started. kl.lastContainerStartedTime.Add(e.ID, time.Now()) } if isSyncPodWorthy(e) { // PLEG event for a pod; sync it. if pod, ok := kl.podManager.GetPodByUID(e.ID); ok { klog.V(2).InfoS(\u0026#34;SyncLoop (PLEG): event for pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;event\u0026#34;, e) handler.HandlePodSyncs([]*v1.Pod{pod}) } else { // If the pod no longer exists, ignore the event. klog.V(4).InfoS(\u0026#34;SyncLoop (PLEG): pod does not exist, ignore irrelevant event\u0026#34;, \u0026#34;event\u0026#34;, e) } } if e.Type == pleg.ContainerDied { if containerID, ok := e.Data.(string); ok { kl.cleanUpContainersInPod(e.ID, containerID) } } // ~~ pkg/kubelet/runtime.go Let\u0026rsquo;s also look at where the PLEG-related health check is added to the kubelet runtime health check with klet.runtimeState.The health check functions, which were added by addHealthCheck(), will be called with the for statement and evaluated. When the health check fails, the error message will be shown with a format like fmt.Errorf(\u0026quot;%s is not healthy: %v\u0026quot;, hc.name, err). Yeah, now we can see where PLEG is not healthy: pleg was last seen active XXmYYs ago; threshold is 3m comes from!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // A health check function should be efficient and not rely on external // components (e.g., container runtime). type healthCheckFnType func() (bool, error) type healthCheck struct { name string fn healthCheckFnType } func (s *runtimeState) addHealthCheck(name string, f healthCheckFnType) { s.Lock() defer s.Unlock() s.healthChecks = append(s.healthChecks, \u0026amp;healthCheck{name: name, fn: f}) } func (s *runtimeState) runtimeErrors() error { s.RLock() defer s.RUnlock() errs := []error{} if s.lastBaseRuntimeSync.IsZero() { errs = append(errs, errors.New(\u0026#34;container runtime status check may not have completed yet\u0026#34;)) } else if !s.lastBaseRuntimeSync.Add(s.baseRuntimeSyncThreshold).After(time.Now()) { errs = append(errs, errors.New(\u0026#34;container runtime is down\u0026#34;)) } for _, hc := range s.healthChecks { if ok, err := hc.fn(); !ok { errs = append(errs, fmt.Errorf(\u0026#34;%s is not healthy: %v\u0026#34;, hc.name, err)) } } if s.runtimeError != nil { errs = append(errs, s.runtimeError) } return utilerrors.NewAggregate(errs) } What\u0026rsquo;s next in PLEG? I found the KEP Kubelet Evented PLEG for Better Performance. It seems still under development for now.\nIsuue: Kubelet Evented PLEG for Better Performance - GitHub References \u0026ldquo;PLEG is not healthy\u0026rdquo; errors on OpenShift nodes. - Red Hat Customer Portal コンセプト - Kubernetes Document Pod Lifecycle Event Generator: Understanding the “PLEG is not healthy” issue in Kubernetes - Red Hat Developer kubernetes/design-proposals-archive - GitHub kubernetes/pkg/kubelet/pleg at release-1.25 - GitHub Isuue: Kubelet Evented PLEG for Better Performance - GitHub ","date":"2023-03-24T09:32:47+09:00","image":"https://blog.nishipy.com/p/understanding-pleg-with-source-code-part-2/pleg_hu198fe2aa39d96d5ee57c047fcc9cf0ca_184542_120x120_fill_box_smart1_3.png","permalink":"https://blog.nishipy.com/p/understanding-pleg-with-source-code-part-2/","title":"Understanding PLEG with source code - Part 2"},{"content":"Note: This article is a translation of 「kubeletのPLEGをソースコード読んで理解したかった」. Kubelet has a lot of components, and one of the most important ones is PLEG, which stands for \u0026ldquo;Pod Lifecycle Event Generator\u0026rdquo;. In this article, we\u0026rsquo;ll look into the implementation to understand how PLEG works.\nNotReady nodes due to \u0026ldquo;PLEG is not healthy\u0026rdquo; In Kubernetes, there are various causes for nodes to become NotReady state. For example, a node may become NotReady with output like PLEG is not healthy: pleg was last seen active 3m20s ago; threshold is 3m0s. PLEG(←?!) was last seen active 3m20s ago; threshold is 3m0s It seems that the last time PLEG(←?!) was active was 3m20s ago; threshold is 3m0s , so it seems that it was judged as abnormal because it exceeded the threshold of 3 minutes. I don\u0026rsquo;t know what it\u0026rsquo;s talking about, what the heck is PLEG\u0026hellip;\nPLEG PLEG is part of kubelet and stands for Pod Lifecycle Event Generator. An overview was found in Concepts in Kubernetes Japanese documentation. Below is its translation with DeepL.\n\u0026gt; Once the desired state is set, the *Kubernetes control plane* using the Pod Lifecycle Event Generator (PLEG) functions to match the current state of the cluster to the desired state. To do so, Kubernetes automatically performs various tasks (e.g., starting or restarting containers, scaling the number of replicas of a particular application, etc.). Okay, that sounds very important. There is an embedded link to PLEG\u0026rsquo;s Design Proposal in this document, but it seems to be out of date and I cannot access it. Apparently, the old Design Proposal has been moved to kubernetes/design-proposals-archive. We can find the PLEG here.\nIt seems to be responsible for talking to Container Runtime and the main loop of kubelet, as shown below.\ncited from: https://github.com/kubernetes/design-proposals-archive/blob/main/node/pod-lifecycle-event-generator.md#overview\nRelisting in PLEG According to the chapter Detect Changes in Container States Via Relisting, it is designed to detect container state changes by a process called \u0026ldquo;relisting\u0026rdquo;.\nPLEG relist all containers periodically to detect container state changes It helps prevent all Pod Workers from polling the container runtime in parallel Therefore only Pod Workers that need Sync will be launched, which is even more efficient Pod Workers FYI, Pod Worker is implemented here. It\u0026rsquo;s also a component of kubelet, and keeps track of operations on pods and ensures each pod is reconciled with the container runtime and other subsystems.\nRuntime Pod Cache We also need to know the Runtime Pod Cache to look into PLEG. The design proposal is here.\ncited from: https://github.com/kubernetes/design-proposals-archive/blob/main/node/runtime-pod-cache.md#runtime-pod-cache\nThe diagram is almost the same as the one we saw for PLEG, but a box named \u0026ldquo;pod cache\u0026rdquo; is added between PLEG and Pod Workers.\nThe Runtime Pod Cache is an in-memory cache that stores the state of all pods and is used to synchronize pods; it is managed by PLEG and acts as a Single Source of Truth (SSOT) for the internal pod status, so that kubelets do not need to directly query the container runtime directly.\nPLEG is responsible for updating the Pod Cache entries, keeping the cache up-to-date at all times. The design seems to be to process in the following order, generating and sending the corresponding Pod Lifecycle Event only when there is a change in the Pod.\nDetect change of container state Inspect the pod for details Update the pod cache with the new PodStatus More information and source code Now that we have understood a little about PLEG, let\u0026rsquo;s look into it in more detail.\nFirst of all, the following article will help a lot to understand it with many diagrams and snippets of source codes. Please note that the information in it is as of Kubernetes 1.14.\nPod Lifecycle Event Generator: Understanding the \u0026ldquo;PLEG is not healthy\u0026rdquo; issue in Kubernetes | Red Hat Developer In the next article\u0026hellip; It might be sufficient to read the above for understanding, but the version of Kubernetes is a little old. Therefore, I\u0026rsquo;ll read the source code of Kubernetes 1.25 in the next.\n","date":"2023-01-28T13:50:47+09:00","image":"https://blog.nishipy.com/p/understanding-pleg-with-source-code-part-1/pleg_hu198fe2aa39d96d5ee57c047fcc9cf0ca_184542_120x120_fill_box_smart1_3.png","permalink":"https://blog.nishipy.com/p/understanding-pleg-with-source-code-part-1/","title":"Understanding PLEG with source code - Part 1"},{"content":"Note: This article is a translation of 「kindでNodeに割り当てるリソースを定義する」. I posted the original on Dec 18 2020, so some information might be old.\nThere are some famous tools to create local Kubernetes clusters. Kind is one of them. In this post, I introduce the way to allocate compute resources(memory and/or CPU) to nodes of Kind.\nAdjust resources for Docker Desktop When using Kind, each node of a Kubernetes cluster is built as a Docker container. Therefore, first, configure the resources available to Docker in the Docker Desktop settings. Set them to about 4 CPU cores and 8GB of memory, for instance.\nCreate a Kind configuration file Next, we create a configuration file for Kind. You can get a general idea by looking at Configuring Your kind Cluster in the documentation. It is a brief description of the cluster.\nDefine multiple nodes in a cluster For now, we will try to configure one Master and one Worker. All we need to do is to define the list under the nodes.\n1 2 3 4 5 kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane - role: worker Define resources for nodes According to this issue, you can define the resource allocation since the Kind cluster is created with kubeadm. However, it does not seem to be a formal method, as you can find a comment like \u0026ldquo;kubelet configuration object is not respected per node in kubeadm currently, only from init\u0026rdquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: system-reserved: memory=4Gi - role: worker kubeadmConfigPatches: - | kind: JoinConfiguration nodeRegistration: kubeletExtraArgs: system-reserved: memory=4Gi Create a cluster with the configuration file Use --config option to specify the configuration file.\n$ kind create cluster --name=demo --config=$HOME/kind-config.yaml Creating cluster \u0026#34;demo\u0026#34; ... ✓ Ensuring node image (kindest/node:v1.19.1) ✓ Preparing nodes ✓ Writing configuration ✓ Starting control-plane \u0026amp;#xfe0f; ✓ Installing CNI ✓ Installing StorageClass ✓ Joining worker nodes Set kubectl context to \u0026#34;kind-demo\u0026#34; You can now use your cluster with: kubectl cluster-info --context kind-demo Not sure what to do next? Check out \u0026lt;https://kind.sigs.k8s.io/docs/user/quick-start/\u0026gt; Confirm the resource allocation You will see that it is properly allocated with kubectl describe node.\n$ kubectl get node\tNAME STATUS ROLES AGE VERSION\tdemo-control-plane Ready master 76s v1.19.1\tdemo-worker Ready \u0026lt;none\u0026gt; 41s v1.19.1\t$ kubectl describe node demo-control-plane\tName: demo-control-plane\t[...]\tAllocatable:\tcpu: 4\tephemeral-storage: 61255492Ki\thugepages-1Gi: 0\thugepages-2Mi: 0\tmemory: 3958900Ki\tpods: 110\t[...]\t$ kubectl describe node demo-worker\tName: demo-worker\t[...]\tAllocatable:\tcpu: 4\tephemeral-storage: 61255492Ki\thugepages-1Gi: 0\thugepages-2Mi: 0\tmemory: 3958900Ki\tpods: 110\t","date":"2023-01-21T10:51:42+09:00","image":"https://blog.nishipy.com/p/resource-allocation-to-kind-nodes/logo_hu3178206b939ba8d9865acabeaf6111f4_85649_120x120_fill_box_smart1_3.png","permalink":"https://blog.nishipy.com/p/resource-allocation-to-kind-nodes/","title":"Resource allocation to Kind nodes "},{"content":"I used to write articles on the WordPress-based blog site, but in 2023 I am trying to migrate to Hugo.\nHugo has a variety of great themes, but I\u0026rsquo;ve decided to use the Hugo Theme Stack.\nWhat is Hugo Theme Stack Hugo Theme Stack is a Card-style Hugo theme designed for bloggers, and one of the best Hugo themes I\u0026rsquo;ve ever seen. It has nice designs, a dark/right theme, and so on.\nHowever, if I understand correctly, it doesn\u0026rsquo;t have a feature to add social media share buttons at the moment I wrote this article. Then, I tried to add them at the bottom of the articles.\nSocial Media Share Buttons for a Hugo Website Please find Social Media Share Buttons for a Hugo Website, which is a perfect guide on how to add social media share buttons for the Hugo themes.\nThe original code can be generated with Sharingbuttons.io. In addition, you have to replace the link and text with {{ .Title }} and {{ .Permalink }} if you want to share the page where you are. Here is my example for Twitter.\n1 2 3 4 5 6 \u0026lt;a class=\u0026#34;resp-sharing-button__link\u0026#34; href=\u0026#34;https://twitter.com/intent/tweet/?text={{ .Title }}\u0026amp;amp;url={{ .Permalink }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34; aria-label=\u0026#34;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small\u0026#34;\u0026gt;\u0026lt;div aria-hidden=\u0026#34;true\u0026#34; class=\u0026#34;resp-sharing-button__icon resp-sharing-button__icon--solid\u0026#34;\u0026gt; \u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5 0-4.55 2.04-4.55 4.54 0 .36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3 0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35 0 12.92-6.92 12.92-12.93 0-.2 0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; Add Social Media Share Buttons for Hugo Theme Stack Referencing the procedures in the previous part, now we can add the buttons for Hugo Theme Stack.\nassets/scss/custom.scss Update custom.scss as follows to support social media share buttons. You just cut and paste the code generated by Sharingbuttons.io.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 // Social media sharing buttons .resp-sharing-button__link, .resp-sharing-button__icon { display: inline-block } .resp-sharing-button__link { text-decoration: none; color: #fff; margin: 0.5em } .resp-sharing-button { border-radius: 5px; transition: 25ms ease-out; padding: 0.5em 0.75em; font-family: Helvetica Neue,Helvetica,Arial,sans-serif } .resp-sharing-button__icon svg { width: 1em; height: 1em; margin-right: 0.4em; vertical-align: top } .resp-sharing-button--small svg { margin: 0; vertical-align: middle } /* Non solid icons get a stroke */ .resp-sharing-button__icon { stroke: #fff; fill: none } /* Solid icons get a fill */ .resp-sharing-button__icon--solid, .resp-sharing-button__icon--solidcircle { fill: #fff; stroke: none } .resp-sharing-button--twitter { background-color: #55acee } .resp-sharing-button--twitter:hover { background-color: #2795e9 } .resp-sharing-button--pinterest { background-color: #bd081c } .resp-sharing-button--pinterest:hover { background-color: #8c0615 } .resp-sharing-button--facebook { background-color: #3b5998 } .resp-sharing-button--facebook:hover { background-color: #2d4373 } .resp-sharing-button--tumblr { background-color: #35465C } .resp-sharing-button--tumblr:hover { background-color: #222d3c } .resp-sharing-button--reddit { background-color: #5f99cf } .resp-sharing-button--reddit:hover { background-color: #3a80c1 } .resp-sharing-button--google { background-color: #dd4b39 } .resp-sharing-button--google:hover { background-color: #c23321 } .resp-sharing-button--linkedin { background-color: #0077b5 } .resp-sharing-button--linkedin:hover { background-color: #046293 } .resp-sharing-button--email { background-color: #777 } .resp-sharing-button--email:hover { background-color: #5e5e5e } .resp-sharing-button--xing { background-color: #1a7576 } .resp-sharing-button--xing:hover { background-color: #114c4c } .resp-sharing-button--whatsapp { background-color: #25D366 } .resp-sharing-button--whatsapp:hover { background-color: #1da851 } .resp-sharing-button--hackernews { background-color: #FF6600 } .resp-sharing-button--hackernews:hover, .resp-sharing-button--hackernews:focus { background-color: #FB6200 } .resp-sharing-button--vk { background-color: #507299 } .resp-sharing-button--vk:hover { background-color: #43648c } .resp-sharing-button--facebook { background-color: #3b5998; border-color: #3b5998; } .resp-sharing-button--facebook:hover, .resp-sharing-button--facebook:active { background-color: #2d4373; border-color: #2d4373; } .resp-sharing-button--twitter { background-color: #55acee; border-color: #55acee; } .resp-sharing-button--twitter:hover, .resp-sharing-button--twitter:active { background-color: #2795e9; border-color: #2795e9; } .resp-sharing-button--linkedin { background-color: #0077b5; border-color: #0077b5; } .resp-sharing-button--linkedin:hover, .resp-sharing-button--linkedin:active { background-color: #046293; border-color: #046293; } .resp-sharing-button--reddit { background-color: #5f99cf; border-color: #5f99cf; } .resp-sharing-button--reddit:hover, .resp-sharing-button--reddit:active { background-color: #3a80c1; border-color: #3a80c1; } assets/scss/partials/layout/article.scss Add the definition of .share-buttons section in .article-footer. Here, it is the same as .article-copyright and .article-lastmod.\n1 2 3 4 5 6 //~~ .article-lastmod, .share-buttons { a { color: var(--body-text-color); } layouts/partials/article/components/share-buttons.html Next, we add a new partial template for share buttons like this. The original one is available at Sharingbuttons.io again. As mentioned above, the link and text are replaced with {{ .Title }} and {{ .Permalink }}.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 \u0026lt;section class=\u0026#34;share-buttons\u0026#34;\u0026gt; \u0026lt;!-- Sharingbutton Twitter --\u0026gt; \u0026lt;a class=\u0026#34;resp-sharing-button__link\u0026#34; href=\u0026#34;https://twitter.com/intent/tweet/?text={{ .Title }}\u0026amp;amp;url={{ .Permalink }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34; aria-label=\u0026#34;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small\u0026#34;\u0026gt;\u0026lt;div aria-hidden=\u0026#34;true\u0026#34; class=\u0026#34;resp-sharing-button__icon resp-sharing-button__icon--solid\u0026#34;\u0026gt; \u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5 0-4.55 2.04-4.55 4.54 0 .36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3 0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35 0 12.92-6.92 12.92-12.93 0-.2 0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;!-- Sharingbutton Facebook --\u0026gt; \u0026lt;a class=\u0026#34;resp-sharing-button__link\u0026#34; href=\u0026#34;https://facebook.com/sharer/sharer.php?u={{ .Permalink }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34; aria-label=\u0026#34;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;resp-sharing-button resp-sharing-button--facebook resp-sharing-button--small\u0026#34;\u0026gt;\u0026lt;div aria-hidden=\u0026#34;true\u0026#34; class=\u0026#34;resp-sharing-button__icon resp-sharing-button__icon--solid\u0026#34;\u0026gt; \u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M18.77 7.46H14.5v-1.9c0-.9.6-1.1 1-1.1h3V.5h-4.33C10.24.5 9.5 3.44 9.5 5.32v2.15h-3v4h3v12h5v-12h3.85l.42-4z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;!-- Sharingbutton LinkedIn --\u0026gt; \u0026lt;a class=\u0026#34;resp-sharing-button__link\u0026#34; href=\u0026#34;https://www.linkedin.com/shareArticle?mini=true\u0026amp;amp;url={{ .Permalink }}\u0026amp;amp;title={{ .Title }}\u0026amp;amp;summary={{ .Title }}\u0026amp;amp;source={{ .Permalink }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34; aria-label=\u0026#34;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--small\u0026#34;\u0026gt;\u0026lt;div aria-hidden=\u0026#34;true\u0026#34; class=\u0026#34;resp-sharing-button__icon resp-sharing-button__icon--solid\u0026#34;\u0026gt; \u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M6.5 21.5h-5v-13h5v13zM4 6.5C2.5 6.5 1.5 5.3 1.5 4s1-2.4 2.5-2.4c1.6 0 2.5 1 2.6 2.5 0 1.4-1 2.5-2.6 2.5zm11.5 6c-1 0-2 1-2 2v7h-5v-13h5V10s1.6-1.5 4-1.5c3 0 5 2.2 5 6.3v6.7h-5v-7c0-1-1-2-2-2z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;!-- Sharingbutton Reddit --\u0026gt; \u0026lt;a class=\u0026#34;resp-sharing-button__link\u0026#34; href=\u0026#34;https://reddit.com/submit/?url={{ .Permalink }}\u0026amp;amp;resubmit=true\u0026amp;amp;title={{ .Title }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34; aria-label=\u0026#34;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;resp-sharing-button resp-sharing-button--reddit resp-sharing-button--small\u0026#34;\u0026gt;\u0026lt;div aria-hidden=\u0026#34;true\u0026#34; class=\u0026#34;resp-sharing-button__icon resp-sharing-button__icon--solid\u0026#34;\u0026gt; \u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M24 11.5c0-1.65-1.35-3-3-3-.96 0-1.86.48-2.42 1.24-1.64-1-3.75-1.64-6.07-1.72.08-1.1.4-3.05 1.52-3.7.72-.4 1.73-.24 3 .5C17.2 6.3 18.46 7.5 20 7.5c1.65 0 3-1.35 3-3s-1.35-3-3-3c-1.38 0-2.54.94-2.88 2.22-1.43-.72-2.64-.8-3.6-.25-1.64.94-1.95 3.47-2 4.55-2.33.08-4.45.7-6.1 1.72C4.86 8.98 3.96 8.5 3 8.5c-1.65 0-3 1.35-3 3 0 1.32.84 2.44 2.05 2.84-.03.22-.05.44-.05.66 0 3.86 4.5 7 10 7s10-3.14 10-7c0-.22-.02-.44-.05-.66 1.2-.4 2.05-1.54 2.05-2.84zM2.3 13.37C1.5 13.07 1 12.35 1 11.5c0-1.1.9-2 2-2 .64 0 1.22.32 1.6.82-1.1.85-1.92 1.9-2.3 3.05zm3.7.13c0-1.1.9-2 2-2s2 .9 2 2-.9 2-2 2-2-.9-2-2zm9.8 4.8c-1.08.63-2.42.96-3.8.96-1.4 0-2.74-.34-3.8-.95-.24-.13-.32-.44-.2-.68.15-.24.46-.32.7-.18 1.83 1.06 4.76 1.06 6.6 0 .23-.13.53-.05.67.2.14.23.06.54-.18.67zm.2-2.8c-1.1 0-2-.9-2-2s.9-2 2-2 2 .9 2 2-.9 2-2 2zm5.7-2.13c-.38-1.16-1.2-2.2-2.3-3.05.38-.5.97-.82 1.6-.82 1.1 0 2 .9 2 2 0 .84-.53 1.57-1.3 1.87z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;!-- Sharingbutton WhatsApp --\u0026gt; \u0026lt;a class=\u0026#34;resp-sharing-button__link\u0026#34; href=\u0026#34;whatsapp://send?text={{ .Title }}%20{{ .Permalink }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34; aria-label=\u0026#34;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;resp-sharing-button resp-sharing-button--whatsapp resp-sharing-button--small\u0026#34;\u0026gt;\u0026lt;div aria-hidden=\u0026#34;true\u0026#34; class=\u0026#34;resp-sharing-button__icon resp-sharing-button__icon--solid\u0026#34;\u0026gt; \u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M20.1 3.9C17.9 1.7 15 .5 12 .5 5.8.5.7 5.6.7 11.9c0 2 .5 3.9 1.5 5.6L.6 23.4l6-1.6c1.6.9 3.5 1.3 5.4 1.3 6.3 0 11.4-5.1 11.4-11.4-.1-2.8-1.2-5.7-3.3-7.8zM12 21.4c-1.7 0-3.3-.5-4.8-1.3l-.4-.2-3.5 1 1-3.4L4 17c-1-1.5-1.4-3.2-1.4-5.1 0-5.2 4.2-9.4 9.4-9.4 2.5 0 4.9 1 6.7 2.8 1.8 1.8 2.8 4.2 2.8 6.7-.1 5.2-4.3 9.4-9.5 9.4zm5.1-7.1c-.3-.1-1.7-.9-1.9-1-.3-.1-.5-.1-.7.1-.2.3-.8 1-.9 1.1-.2.2-.3.2-.6.1s-1.2-.5-2.3-1.4c-.9-.8-1.4-1.7-1.6-2-.2-.3 0-.5.1-.6s.3-.3.4-.5c.2-.1.3-.3.4-.5.1-.2 0-.4 0-.5C10 9 9.3 7.6 9 7c-.1-.4-.4-.3-.5-.3h-.6s-.4.1-.7.3c-.3.3-1 1-1 2.4s1 2.8 1.1 3c.1.2 2 3.1 4.9 4.3.7.3 1.2.5 1.6.6.7.2 1.3.2 1.8.1.6-.1 1.7-.7 1.9-1.3.2-.7.2-1.2.2-1.3-.1-.3-.3-.4-.6-.5z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/section\u0026gt; layouts/partials/article/components/footer.html In the end, add the partial template to footer.html for article pages.\n1 2 3 4 5 // ~~ \u0026lt;!-- Social media share buttons --\u0026gt; {{ partial \u0026#34;article/components/share-buttons\u0026#34; . }} \u0026lt;/footer\u0026gt; Now we have social media share buttons on the footer of articles! They are displayed below, right?\n","date":"2023-01-14T12:04:42+09:00","image":"https://blog.nishipy.com/p/social-media-share-buttons-for-hugo-theme-stack/share-buttons_hu81c5837a67928c7bc03997a4103c9301_18652_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.nishipy.com/p/social-media-share-buttons-for-hugo-theme-stack/","title":"Social Media Share Buttons for Hugo Theme Stack"},{"content":"見逃さない人 おそろしく速い手刀、オレでなきゃ見逃しちゃうね\nA person who does not miss that Such terrifying speed. No one except me could have seen that. ","date":"2023-01-09T12:47:43+09:00","permalink":"https://blog.nishipy.com/p/hello-world/","title":"Hello World"}]